[["index.html", "Introduction to Optimization Preface", " Introduction to Optimization Apurva Nakade 2022-07-11 Preface These are class notes for a quarter long course on Introduction to Optimization (Math 368) at Northwestern University taught during the Winter and Spring of 2022. For the most part, these notes talk about linear programming and duality theory, and only towards the very end we discuss non-linear programming. The class is aimed at upper-level undergrads whove completed a basic sequence in linear algebra and calculus. The prerequisites for the linear programming part are Gaussian elimination and basic theorem proving. For the non-linear programming part, youll need to know basic properties of gradients and directional derivatives. These notes are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. Please feel free to fork and modify the Github repo or use these notes as-is. Any suggestions and improvements to the notes are welcome and greatly appreciated. You can send me an email or open an issue on Github. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction An optimization problem consists of maximizing or minimizing a real function over a set of input values from within an allowed set. \\[\\begin{align*} \\mbox{optimize: } &amp; f(x_1, \\dots, x_n) \\\\ \\mbox{subject to: } &amp; (x_1, \\dots, x_n) \\in D. \\end{align*}\\] These kinds questions arise in all quantitative disciplines from computer science and engineering to operations research and economics. As stated, this question is too broad. To make any meaningful analysis, we need to make several simplifying assumptions about \\(f\\) and \\(D\\). To begin with, well assume that the function \\(f\\) is linear and the constraint set \\(D\\) is described using linear inequalities. The study of this problem is called Linear Programming. Despite the simplicity of the linear programming setup, or perhaps because of it, LP is one of the most commonly used models for optimization problems. Lets start with an example before getting into the precise definitions. Example 1.1 A bond portfolio manager has $100,000 to allocate to two different bonds; one corporate and one government bond. The corporate bond has a yield of 4%, a maturity of 3 years and an A rating from a rating agency that is translated into a numerical rating of 2 for computational purposes. In contrast, the government bond has a yield of 3%, a maturity of 6 years and rating of Aaa with the corresponding numerical rating of 1 (lower numerical ratings correspond to higher quality bonds). The portfolio manager would like to allocate her funds so that the average rating for the portfolio is no worse than Aa (numerical equivalent 1.5) and average maturity of the portfolio is at most 3.6 years. Any amount not invested in the two bonds will be kept in a cash account that is assumed to earn no interest for simplicity and does not contribute to the average rating or maturity computations. How should the manager allocate her funds between these two bonds to achieve her objective of maximizing the annual yield from this investment? (Cornuéjols, Peña, and Tütüncü 2018) Corporate Government Constraints Yield 4% 3% Maturity 3 6 3.6 Rating A = 2 Aaa = 1 1.5 Allocations ?? ?? 100,000 We can model the above problem as follows: \\[\\begin{equation} \\begin{array}{rrrrrr} \\mbox{maximize:} &amp; 4x &amp; + &amp; 3y \\\\ \\mbox{subject to:} &amp; 3x &amp; + &amp; 6y &amp; \\le &amp; 3.6 \\\\ &amp; 2x &amp; + &amp; y &amp; \\le &amp; 1.5 % &amp; x &amp; + &amp; y &amp; \\le &amp; 1 \\\\ % &amp; x &amp; , &amp; y &amp; \\ge &amp; 0, \\end{array} \\tag{1.1} \\end{equation}\\] where \\(x\\), \\(y\\) are the percentages of funds allocated to corporate and government bonds, respectively, and the objective function when multiplied by $100,000 gives us the net yield. This is an example of a linear program. Note that we cannot subtract inequalities the same way that we subtract equations. So to get started, let us assume that both the inequalities are in fact equations. We can solve the resulting system, \\[\\begin{equation*} \\begin{array}{rrrrrl} &amp; 3x &amp; + &amp; 6y &amp; = &amp; 3.6 \\\\ &amp; 2x &amp; + &amp; y &amp; = &amp; 1.5, \\end{array} \\end{equation*}\\] to obtain \\(x = 0.6\\) and \\(y = 0.3\\).For this solution \\(x + y = 0.9\\) which is less that 1, meaning that were not investing all the available funds. This seems less than ideal! Surely, we must invest all the funds to get maximum yield!! To complicate things further, notice that the linear program (1.1) is incomplete and the complete linear program that models the problem is as follows: \\[\\begin{equation} \\begin{array}{rrrrrl} \\mbox{maximize:} &amp; 4x &amp; + &amp; 3y \\\\ \\mbox{subject to:} &amp; 3x &amp; + &amp; 6y &amp; \\le &amp; 3.6 \\\\ &amp; 2x &amp; + &amp; y &amp; \\le &amp; 1.5 \\\\ &amp; x &amp; + &amp; y &amp; \\le &amp; 1 \\\\ &amp; x &amp; &amp; &amp; \\ge &amp; 0 \\\\ &amp; &amp; &amp; y &amp; \\ge &amp; 0. \\end{array} \\tag{1.2} \\end{equation}\\] The solution \\((x, y) = (0.6, 0.3)\\) is obtained by changing the first two inequalities to equations. We could have switched some other set of inequalities to equations and solved for a different solution. We would then need to compare the yields for each of these solutions and find the one that maximizes it. However, now there is an additional issue that needs to be dealt with carefully. Solving these modified equations might produce a solution that does not satisfy the original constraints. For example, solving the system of equations \\[\\begin{equation*} \\begin{array}{rrrrrl} &amp; 2x &amp; + &amp; y &amp; = &amp; 1.5 \\\\ &amp; x &amp; &amp; &amp; = &amp; 0 \\end{array} \\end{equation*}\\] gives us \\((x, y) = (0, 1.5)\\) which breaks the constraints \\(3x + 6y \\le 3.6\\) and \\(x + y \\le 1\\). So, not only do we need to compare the yields at these solutions but we also need to make sure that they satisfy all the original constraints. As you can imagine, this naive approach becomes unwieldy for large linear programs. It is not even clear that changing inequalities to equations is the right approach to solving such a problem.1 For this particular problem, we can use a graphing calculator to solve the problem as there are only two variables. The following figure shows the region described by the various constraints in the first quadrant (\\(x \\ge 0, y \\ge 0\\)). The points within the quadrilateral formed by the overlap of all three constraint regions satisfy all the constraints. The level sets of the objective function are straight lines of the form \\(4x + 3y = c\\). We can see that the largest value of \\(c\\) for which these level sets intersect the feasible region is \\(c = 3.3\\%\\) for \\((x, y) = (0.6, 0.3)\\). Our original solution was indeed correct! And it is not possible to invest more money and increase the yield!! Exercise 1.1 What are some possible ways to relax the constraints in the above problem so that all the money can be invested? What is the improved annual yield? Exercise 1.2 The following exercises review some basic facts about inequalities: Find constants \\(a, b, c, d\\) such that \\(a &lt; b\\), \\(c &lt; d\\), \\(a - c &lt; b - d\\), and \\(ac &lt; bd\\). Find constants \\(a, b, c, d\\) such that \\(a &lt; b\\), \\(c &lt; d\\), \\(a - c &lt; b - d\\), and \\(ac &gt; bd\\). Find constants \\(a, b, c, d\\) such that \\(a &lt; b\\), \\(c &lt; d\\), \\(a - c &gt; b - d\\), and \\(ac &lt; bd\\). Find constants \\(a, b, c, d\\) such that \\(a &lt; b\\), \\(c &lt; d\\), \\(a - c &gt; b - d\\), and \\(ac &gt; bd\\). Prove that if \\(a, b, c, d\\) are constants such that \\(a &lt; b\\) and \\(c &lt; d\\), then \\(a + c &lt; b + d\\). Why does this proof fail when we try to subtract the two inequalities instead of adding them? There are two main families of efficient algorithms for solving linear programs: simplex method(s) (Chapter 3) and interior point methods (Chapter 12). The simplex method is a combinatorial algorithm that essentially performs the above analysis systematically. Interior point methods modify gradient descent algorithms to search for an optimal solution within the feasible region. Well analyze the simplex method in great detail and only briefly introduce a basic interior point method. References "],["standard-linear-program.html", "Chapter 2 Standard Linear Program", " Chapter 2 Standard Linear Program We will describe the simplex method for solving a special kind of linear program called standard linear program. We will see later that every linear program can be standardized (Chapter 6) and hence this method is sufficient for solving any linear program. Definition 2.1 A standard linear program is an optimization problem of the following form: \\[\\begin{equation} \\begin{array}{lrrrrrrrrr} \\mbox{maximize: } &amp; c_0 &amp; + &amp; c_1 x_1 &amp; + &amp; \\dots &amp; + &amp; c_n x_n &amp; \\\\ \\mbox{subject to: } &amp; &amp; &amp; a_{11} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{1n} x_n &amp; \\leq &amp; b_1 \\\\ &amp; &amp; &amp; a_{21} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{2n} x_n &amp; \\leq &amp; b_2 \\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ &amp; &amp; &amp; a_{m1} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{mn} x_n &amp; \\leq &amp; b_m \\\\ &amp; &amp; &amp; x_1, &amp; x_2, &amp; \\dots &amp;, &amp; x_n &amp; \\geq &amp; 0 \\end{array} \\tag{2.1} \\end{equation}\\] where \\(c_i\\), \\(a_{ij}\\), and \\(b_j\\) are real constants. The variables \\(x_1, \\dots, x_n\\) are called decision variables. A tuple \\((x_1, \\dots, x_n)\\) that satisfy all the constraints is called a feasible solution and the set of all feasible solutions is called the feasible region. Example 2.1 Equation (1.2) is an example of a standard linear program with 2 decision variables and 3 constraints. The feasible region is a quadrilateral with vertices \\((0,0)\\), \\((0.75, 0)\\), \\((0.6, 0.3)\\), and \\((0, 0.6)\\). Definition 2.2 For each constraint of the standard linear program (2.1), we introduce a slack variable by subtracting the LHS from the RHS as follows. \\[\\begin{equation} \\begin{array}{lrrrrrrrrr} w_1 &amp; = &amp; b_1 &amp; - &amp; a_{11} x_1 &amp; - &amp; \\dots &amp; - &amp; a_{1n} x_n \\\\ w_2 &amp; = &amp; b_2 &amp; - &amp; a_{21} x_1 &amp; - &amp; \\dots &amp; - &amp; a_{2n} x_n \\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ w_m &amp; = &amp; b_m &amp; - &amp; a_{m1} x_1 &amp; - &amp; \\dots &amp; - &amp; a_{mn} x_n \\end{array} \\tag{2.2} \\end{equation}\\] We can think of the slack variable \\(w_i\\) as measuring the slackness in the \\(i^{th}\\) constraint. The \\(i^{th}\\) constraint is strictly met exactly when \\(w_i\\) is zero. Using the slack variables, the linear program (2.1) can be succinctly rewritten as: \\[\\begin{equation*} w_1, \\dots, w_m, x_1, \\dots, x_n \\geq 0. \\end{equation*}\\] Example 2.2 The slack variables for the linear program (1.2) are as follows: \\[\\begin{equation*} \\begin{array}{rlllllll} w_1 &amp; = &amp; 3.6 &amp; - &amp; 3x &amp; - &amp; 6y \\\\ w_2 &amp; = &amp; 1.5 &amp; - &amp; 2x &amp; - &amp; y \\\\ w_3 &amp; = &amp; 1 &amp; - &amp; x &amp; - &amp; y. \\end{array} \\end{equation*}\\] In terms of these slack variables, the constraints can be rewritten as \\(x, y, w_1, w_2, w_3 \\ge 0\\) and the boundaries of the feasible region are given by \\(x = 0, y = 0, w_1 = 0, w_2 = 0, w_3 = 0\\). Definition 2.3 A basic feasible solution (BFS) to the standard linear program (2.1) is defined as a feasible solution at which at least \\(n\\) decision or slack variables are zero. A BFS where exactly \\(n\\) decision or slack variables are zero is called non-degenerate. A BFS where more than \\(n\\) basic or decision variables are 0 is called degenerate. At a non-degenerate BFS, the \\(n\\) variables that equal 0 are called non-basic and the remaining \\(m\\) variables are called basic. At a degenerate BFS, we choose some \\(n\\) vanishing variables to be non-basic and call the rest of the variables basic. The basic feasible solutions are precisely the solutions obtained by changing \\(n\\) or more inequalities to equations and solving the resulting simultaneous system of equations. As there are \\(n\\) variables, we need at least \\(n\\) linear equations in our system for the solution to be a single point. A degeneracy occurs when more than \\(n\\) equations have a single solution, for example, when three lines in \\(\\mathbb{R}^2\\) or four planes in \\(\\mathbb{R}^3\\) intersecting at a single point. It is useful to think of the non-degenerate basic feasible solutions as vertices of the feasible region. The degenerate BFS do not have a nice geometric description. Exercise 2.1 Show that if \\(k &lt; n\\) then the solution set of a system of \\(k\\) linear equations in \\(n\\) variables cannot be a single point. Example 2.3 The feasible region for the linear program (1.2) has four non-degenerate basic feasible solutions: \\((0,0)\\), \\((0.75, 0)\\), \\((0.6, 0.3)\\), and \\((0, 0.6)\\). and the optimal solution is attained at the BFS \\((0.6, 0.3)\\). At the origin, the non-basic variables are \\(x, y\\) and the basic variables are \\(w_1, w_2, w_3\\). At the optimal solution, the non-basic variables are \\(w_1, w_2\\) and the basic variables are \\(x, y, w_3\\). If we replace the constraint \\(x + y \\le 1\\) with \\(x + y \\le 0.9\\) then the BFS \\((0.6, 0.3)\\) becomes degenerate as here three (slack) variables \\(w_1, w_2, w_3\\) are zero. It turns out that it is sufficient to search for optimal solutions within the set of basic feasible solutions, which is finite, as made precise by the following theorem. Theorem 2.1 (Fundamental theorem of linear programming) For a standard linear program, exactly one of the following holds: There is no feasible solution. In this case, we call the linear program infeasible. The objective value can grow arbitrary large on the feasible region. In this case, we call the linear program unbounded. There is an optimal solution. In this case, we can further say that then there is a basic feasible solution which is optimal. We will assume this theorem without proof. The hardest step in the proof is showing that some basic feasible solution is optimal (if an optimal solution exists) when there are degenerate basic feasible solutions. Exercise 2.2 Find an upper bound on the set of basic feasible solutions for a standard linear program with \\(n\\) decision variables and \\(m\\) equations. "],["the-simplex-method.html", "Chapter 3 The Simplex Method 3.1 The Simplex Step 3.2 Tableau Notation", " Chapter 3 The Simplex Method The simplex method is an iterative process for finding an optimal basic feasible solution to a standard linear program. It starts at some BFS and in each step moves to an adjacent one with a higher objective value. The following picture shows one possible run of the simplex method for the linear program (1.2). Figure 3.1: A possible run of the simplex algorithm. In each step, one non-basic variable enters the set of basic variables and one basic variable leaves the set of basic variables. The table below explains how these sets are getting updated in the sample simplex method run in Figure 3.1. Leaving variable Entering variable Basic variables Non-basic variables Start \\(\\{w_1, w_2, w_3\\}\\) \\(\\{x, y\\}\\) Step 1 \\(w_2\\) \\(x\\) \\(\\{w_1, x, w_3\\}\\) \\(\\{w_2, y\\}\\) Step 2 \\(w_1\\) \\(y\\) \\(\\{y, x, w_3\\}\\) \\(\\{w_2, w_1\\}\\) Using this terminology, our goal at each step is reduced to finding which variable enters and which variable leaves. To do this algebraically, well introduce the notion of dictionaries. Definition 3.1 The dictionary at a basic feasible solution is the set of equations describing the objective function and the constraints in terms of the non-basic variables. Example 3.1 Consider (1.2) again. At the origin, the non-basic variables are \\(x, y\\) and hence the initial dictionary is: \\[\\begin{equation} \\begin{array}{rlrrrrr} \\mbox{objective} &amp; = &amp; 0 &amp; + &amp; 4x &amp; + &amp; 3y \\\\ w_1 &amp; = &amp; 3.6 &amp; - &amp; 3x &amp; - &amp; 6y \\\\ w_2 &amp; = &amp; 1.5 &amp; - &amp; 2x &amp; - &amp; y \\\\ w_3 &amp; = &amp; 1 &amp; - &amp; x &amp; - &amp; y. \\end{array} \\tag{3.1} \\end{equation}\\] After the first step of the simplex algorithm, the non-basic variables are \\(w_2, y\\). We can write \\(x\\) in terms of \\(w_2\\) to get \\[\\begin{equation*} x = 0.75 - 0.5 w_2 - 0.5 y \\end{equation*}\\] We can substitute this into the initial dictionary to get the dictionary after the first step: \\[\\begin{equation} \\begin{array}{rlrrrrr} \\mbox{objective} &amp; = &amp; 3 &amp; + &amp; (-2)w_2 &amp; + &amp; y \\\\ w_1 &amp; = &amp; 1.35 &amp; - &amp; (-1.5) w_2 &amp; - &amp; 4.5y \\\\ x &amp; = &amp; 0.75 &amp; - &amp; 0.5 w_2 &amp; - &amp; 0.5y \\\\ w_3 &amp; = &amp; 0.25 &amp; - &amp; (-0.5) w_2 &amp; - &amp; 0.5y. \\end{array} \\tag{3.2} \\end{equation}\\] Finally, the non-basic variables at the optimal solution are \\(w_1, w_2\\). We can repeat the above process and get the dictionary for the optimal solution: \\[\\begin{equation} \\begin{array}{rlrrrrr} \\mbox{objective} &amp; = &amp; 3.3 &amp; + &amp; (-5/3)w_2 &amp; + &amp; (-2/9)w_1 \\\\ y &amp; = &amp; 0.3 &amp; - &amp; (-1/3) w_2 &amp; - &amp; 2/9 w_1 \\\\ x &amp; = &amp; 0.6 &amp; - &amp; 2/3 w_2 &amp; - &amp; (-1/9) w_1 \\\\ w_3 &amp; = &amp; 0.1 &amp; - &amp; (-1/3) w_2 &amp; - &amp; (-1/9) w_1. \\end{array} \\tag{3.3} \\end{equation}\\] Remark. From the dictionary, one can extract the set of basic variables and the set of non-basic variables by looking at the variables appearing on the LHS and RHS, respectively. Furthermore, by setting the non-basic variables to 0, we obtain the very useful fact that the values of the basic variables are simply the constants \\(b_i\\), and the value of the objective function is the constant \\(c_0\\). For example, from the final dictionary above, we can immediately see that \\(x = 0.6\\), \\(y = 0.3\\), \\(w_3 = 0.1\\), and the objective value is \\(3.3%\\) (and non-basic variables \\(w_1\\) and \\(w_2\\) are both zero) at the optimal solution. 3.1 The Simplex Step Suppose we are at a BFS. Let \\(\\bar{w}_1, \\dots, \\bar{w}_m\\) be the basic variables, \\(\\bar{x}_1, \\dots, \\bar{x}_n\\) be the non-basic variables, \\(\\bar{c}_j\\), \\(\\bar{b}_i\\), and \\(\\bar{a}_{ij}\\) be the constants appearing in the dictionary at the BFS so that the dictionary is as follows: \\[\\begin{equation} \\begin{array}{rrrrrrrrrr} \\mbox{objective} &amp; = &amp; \\bar{c}_0 &amp; + &amp; \\bar{c}_1\\bar{x}_1 &amp; + &amp; \\dots &amp; + &amp; \\bar{c}_n\\bar{x}_n \\\\ \\bar{w}_1 &amp; = &amp; \\bar{b}_1 &amp; - &amp; \\bar{a}_{11} \\bar{x}_1 &amp; - &amp; \\dots &amp; - &amp; \\bar{a}_{1n} \\bar{x}_n \\\\ \\bar{w}_2 &amp; = &amp; \\bar{b}_2 &amp; - &amp; \\bar{a}_{21} \\bar{x}_1 &amp; - &amp; \\dots &amp; - &amp; \\bar{a}_{2n} \\bar{x}_n \\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ \\bar{w}_m &amp; = &amp; \\bar{b}_m &amp; - &amp; \\bar{a}_{m1} \\bar{x}_1 &amp; - &amp; \\dots &amp; - &amp; \\bar{a}_{mn} \\bar{x}_n \\end{array} \\tag{3.4} \\end{equation}\\] 3.1.1 Entering variable We want to move to an adjacent BFS with a higher objective value. The current objective function is \\[\\begin{align*} \\bar{c}_0 + \\bar{c}_1\\bar{x}_1 + \\dots + \\bar{c}_n\\bar{x}_n. \\end{align*}\\] and the current objective value is \\(\\bar{c}_0\\) as all the non-basic variables \\(\\bar{x}_j\\) are zero. We can move away from the current BFS by increasing one of the non-basic variables \\(\\bar{x}_j\\) from zero to a positive value. The objective value will increase precisely when \\(\\bar{c}_j &gt; 0\\). This then is the criterion for choosing the entering variable. We can think of this as choosing the direction of the simplex step. Proposition 3.1 (Entering variable) A non-basic variable \\(\\bar{x}_j\\) can be entering if \\(\\bar{c}_j &gt; 0\\). Example 3.2 In the dictionary (3.1), the objective function is \\(4x + 3y\\). Hence, both \\(x\\) and \\(y\\) can be chosen as entering variables. We can see that there are two different paths along the boundary of the feasible region from the origin to the optimal solution. In the dictionary (3.2), the objective function is \\(3 + (-2)w_2 + y\\) and only \\(y\\) can be the entering variable. In dictionary (3.3), the objective function is \\(3.3 + (-5/3)w_2 + (-2/9)w_1\\) and no variable can enter. 3.1.2 Leaving Variable Suppose we choose \\(\\bar{x}_j\\) as our entering variable. This determines the direction of the simplex step. We next need to figure out the amount by which to increase \\(\\bar{x}_j\\) without leaving the feasible region. The basic variable \\(\\bar{w}_i\\) is related to \\(\\bar{x}_j\\) by the following relation \\[\\begin{align*} \\bar{w}_i = \\bar{b}_i - \\bar{a}_{ij} \\bar{x}_j - \\sum \\limits_{k = 1, k \\neq j}^{n} \\bar{a}_{ik} \\bar{x}_k. \\end{align*}\\] As we increase (only) \\(\\bar{x}_j\\), the quantity within the summation remains zero. If \\(a_{ij} &gt; 0\\), then as we increase \\(\\bar{x}_j\\) the basic variable \\(\\bar{w}_j\\) will decrease. Because we want all the variables to be non-negative, we must always have \\(\\bar{w}_i = \\bar{b}_i - \\bar{a}_{ij} \\bar{x}_j \\ge 0\\). This condition must hold true for all such \\(\\bar{w}_i\\). Hence, we get the following criterion for choosing the leaving variable. Proposition 3.2 (Leaving variable) Suppose \\(\\bar{x}_j\\) is the entering variable. The basic variable \\(\\bar{w}_i\\) can be chosen to be the leaving variable if \\[\\begin{align*} i = {\\arg \\min} _{\\bar{a}_{ij} &gt; 0} \\dfrac{\\bar{b}_i}{\\bar{a}_{ij}}. \\end{align*}\\] If we choose \\(\\bar{w}_i\\) to be leaving then after the simplex step, \\(\\bar{w}_i\\) decreases to \\(0\\), \\(\\bar{x}_j\\) increases to \\(\\min_{\\bar{a}_{ij} &gt; 0} {\\bar{b}_i}/{\\bar{a}_{ij}}\\), and the objective value increases by \\(\\bar{c}_j \\min_{\\bar{a}_{ij} &gt; 0} {\\bar{b}_i}/{\\bar{a}_{ij}}\\). Example 3.3 In the dictionary (3.1), if we choose \\(x\\) to be our entering variable then we need to compare the following ratios: \\(i\\) \\(\\bar{a}_{ij}\\) \\(\\bar{b}_i\\) \\(\\bar{b}_i/\\bar{a}_{ij}\\) 1 3 3.6 1.2 2 2 1.5 0.75 3 1 1 1 We can see that the smallest ratio is obtained for \\(w_2\\). Hence, it is the only candidate for the leaving variable. 3.2 Tableau Notation When manipulating a linear system of equations, one can forget the variables and perform manipulations on the coefficients using matrices. The same is true for linear programs. We start by rewriting the constraints in the dictionary with all the variables on the LHS and all the constants on the RHS: \\[\\begin{equation} \\begin{array}{rrrrrrrrrrrrrr} \\bar{a}_{11} \\bar{x}_1 &amp; + &amp; \\dots &amp; + &amp; \\bar{a}_{1n} \\bar{x}_n &amp; + &amp; \\bar{w}_1 &amp; &amp; &amp; &amp; &amp; &amp; = &amp; \\bar{b}_1\\\\ \\bar{a}_{21} \\bar{x}_1 &amp; + &amp; \\dots &amp; + &amp; \\bar{a}_{2n} \\bar{x}_n &amp; &amp; &amp; + &amp; \\bar{w}_2 &amp; &amp; &amp; &amp; = &amp; \\bar{b}_2\\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ \\bar{a}_{m1} \\bar{x}_1 &amp; + &amp; \\dots &amp; + &amp; \\bar{a}_{mn} \\bar{x}_n &amp; &amp; &amp; &amp; &amp; &amp; + &amp; \\bar{w}_m &amp; = &amp; \\bar{b}_m\\\\ \\end{array} \\tag{3.5} \\end{equation}\\] This can be encoded using the following augmented matrix: \\[\\begin{equation*} \\begin{array}{rrrrrrrrrrr|r} \\bar{a}_{11} &amp; &amp; \\dots &amp; &amp; \\bar{a}_{1n} &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp;\\bar{b}_1\\\\ \\bar{a}_{21} &amp; &amp; \\dots &amp; &amp; \\bar{a}_{2n} &amp; &amp; &amp; 1 &amp; &amp; &amp; &amp;\\bar{b}_2\\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ \\bar{a}_{m1} &amp; &amp; \\dots &amp; &amp; \\bar{a}_{mn} &amp; &amp; &amp; &amp; &amp; &amp; 1 &amp;\\bar{b}_m\\\\ \\end{array} \\end{equation*}\\] We add back the objective function, but because of a quirk of algebra we need to add the objective function coefficients as follows: \\[\\begin{equation*} \\begin{array}{rrrrrrrrrrr|r} \\bar{a}_{11} &amp; &amp; \\dots &amp; &amp; \\bar{a}_{1n} &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp;\\bar{b}_1\\\\ \\bar{a}_{21} &amp; &amp; \\dots &amp; &amp; \\bar{a}_{2n} &amp; &amp; &amp; 1 &amp; &amp; &amp; &amp;\\bar{b}_2\\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ \\bar{a}_{m1} &amp; &amp; \\dots &amp; &amp; \\bar{a}_{mn} &amp; &amp; &amp; &amp; &amp; &amp; 1 &amp;\\bar{b}_m\\\\ \\hline \\bar{c}_1 &amp; &amp; \\dots &amp; &amp; \\bar{c}_{n} &amp; 0 &amp; &amp; 0 &amp; \\dots &amp; &amp; 0 &amp;-\\bar{c}_0 \\end{array} \\end{equation*}\\] The columns in this augmented matrix correspond to the variables \\(\\bar{x}_j\\) and \\(\\bar{w}_i\\). The columns with the pivots correspond to the basic variables. If \\(\\bar{x}_j\\) is the entering variable and \\(\\bar{w}_i\\) is the leaving variable, then we perform elementary row operations and turn the entry \\(\\bar{a}_{ij}\\) into a pivot for its column to obtain the tableau at the next BFS. Hence, the simplex step is also called the pivot step. Example 3.4 The tableau corresponding to the dictionary (3.1) is as follows: \\[\\begin{equation*} \\begin{array}{lllll|l} 3 &amp; 6 &amp; 1 &amp; 0 &amp; 0 &amp; 3.6 \\\\ \\boxed{2} &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1.5 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ \\hline 4 &amp; 3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\end{equation*}\\] If we choose \\(x\\) as the entering variable and \\(w_2\\) as the leaving variable then we need to pivot about the entry \\(a_{21}\\) using elementary row operations to get the following tableau: \\[\\begin{equation*} \\begin{array}{lllll|l} 0 &amp; 4.5 &amp; 1 &amp; -1.5 &amp; 0 &amp; 1.35 \\\\ \\boxed{1} &amp; 0.5 &amp; 0 &amp; 0.5 &amp; 0 &amp; 0.75 \\\\ 0 &amp; 0.5 &amp; 0 &amp; -0.5 &amp; 1 &amp; 0.25 \\\\ \\hline 0 &amp; 1 &amp; 0 &amp; -2 &amp; 0 &amp; -3 \\end{array} \\end{equation*}\\] which corresponds to the dictionary (3.2). "],["initialization.html", "Chapter 4 Initialization 4.1 Auxiliary Linear Program 4.2 Combined tableau", " Chapter 4 Initialization So far, weve seen how to move from one basic feasible solution to an adjacent one with a higher objective value. To have a complete algorithm for solving standard linear programs, we still need a method that finds some BFS. If were lucky, the origin is a basic feasible solution and we can start our simplex method here. However, this is not always the case. The following proposition follows easily from definitions. Proposition 4.1 The origin is a basic feasible solution of the standard linear program (2.1) if and only if \\(b_i \\ge 0\\) for all \\(1 \\le i \\le m.\\) When the origin is not a BFS, we need a process to find one. This is called the Initialization Phase or Phase I. We will do this by creating an auxiliary linear program. 4.1 Auxiliary Linear Program Definition 4.1 We say that a linear program is feasible if its feasible region is non-empty. The initialization phase determines if a standard linear program is feasible and if it is then it finds a BFS by solving the following auxiliary linear program. Definition 4.2 The auxiliary linear program of the standard linear program (2.1) is defined as follows: \\[\\begin{equation} \\begin{array}{lrrrrrrrrrrr} \\mbox{maximize: } &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; - &amp; x_0 &amp; \\\\ \\mbox{subject to: } &amp; &amp; &amp; a_{11} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{1n} x_n &amp; - &amp;x_0 &amp; \\leq &amp; b_1 \\\\ &amp; &amp; &amp; a_{21} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{2n} x_n &amp; - &amp;x_0 &amp; \\leq &amp; b_2 \\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ &amp; &amp; &amp; a_{m1} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{mn} x_n &amp; - &amp;x_0 &amp; \\leq &amp; b_m \\\\ &amp; &amp; &amp; x_1, &amp; x_2, &amp; \\dots &amp;, &amp; x_n &amp; , &amp; x_0 &amp; \\geq &amp; 0. \\end{array} \\tag{4.1} \\end{equation}\\] Were introducing a new variable \\(-x_0\\), adding \\(-x_0\\) to each of the constraint LHS, and changing the objective function to \\(-x_0\\). To understand the auxiliary linear program, we rewrite it in the following non-standard form: \\[\\begin{equation*} \\begin{array}{lrrrrrrrrrrrrr} \\mbox{minimize: } &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; x_0 &amp; \\\\ \\mbox{subject to: } &amp; &amp; &amp; a_{11} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{1n} x_n &amp; \\leq &amp; b_1 &amp; + &amp; x_0 \\\\ &amp; &amp; &amp; a_{21} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{2n} x_n &amp; \\leq &amp; b_2 &amp; + &amp; x_0 \\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\\\ &amp; &amp; &amp; a_{m1} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{mn} x_n &amp; \\leq &amp; b_m &amp; + &amp; x_0 \\\\ &amp; &amp; &amp; x_1, &amp; x_2, &amp; \\dots &amp;, &amp; x_n &amp; , &amp; x_0 &amp; \\geq &amp; 0 \\end{array} \\end{equation*}\\] We can interpret \\(x_0\\) as a relaxation of the constraints. Solving the auxiliary linear program is equivalent to asking - what is the smallest constraint relaxation necessary to make the primary linear program feasible?. The primary linear program is feasible if and only if no relaxation is necessary. The following exercises make this more precise. Exercise 4.1 Show that the auxiliary linear program (4.1) is always feasible by explicitly constructing a feasible solution. Exercise 4.2 Show that the objective values of the auxiliary linear program (4.1) are bounded from above by 0. Using the extreme value theorem, conclude that (4.1) always has an optimal solution. Finally, the following theorem (proof left as an exercise) establishes an explicit connection between optimal solutions of the auxiliary linear program and feasible solutions of the standard linear program. Theorem 4.1 Suppose \\(x_i = k_i\\) for \\(0 \\le i \\le m\\) is an optimal solution of the auxiliary linear program (4.1). Then, the standard linear program (2.1) is feasible if and only if \\(k_0 = 0\\). In this case, \\(x_i = k_i\\) for \\(0 \\le i \\le m\\) is a basic feasible solution of (2.1). Our problem is now reduced to determining the optimal solution of the auxiliary linear program, which well do using the simplex method itself! However, even for the auxiliary linear program the origin is not necessarily in the feasible region so we cannot use the simplex steps to find an optimal solution. But, in this case, there is a trick to make all the \\(b_i \\ge 0\\) by performing a single elementary row operation as described in the following exercises. Exercise 4.3 Suppose some \\(b_i\\) is negative. Without any loss of generality, assume that \\(b_1\\) is the smallest among all the \\(b_i\\) (i.e. the most negative). Show that after the pivot step in tableau for the auxiliary linear program (4.1) about the column corresponding to \\(x_0\\) and the row corresponding to \\(w_1\\), all the \\(b_i\\)s become positive. To summarize: If the origin is not a vertex of the feasible region, then the method of solving the standard linear program (2.1), starting with Phase I, is as follows: Form the auxiliary linear program and its tableau. Perform a pivot operation about the entry in the column corresponding to the variable \\(x_0\\) and the row corresponding to the most negative \\(b_i\\). This results in a dictionary where all the \\(b_i\\)s are now non-negative. Solve the auxiliary linear program by repeatedly performing pivot steps. Suppose \\(x_i = k_i\\) for \\(0 \\le i \\le m\\) is an optimal solution of the auxiliary linear program. If \\(k_0 \\neq 0\\), then we halt as the primary linear program is not feasible. If \\(k_0 = 0\\), then we proceed to Phase II. We solve the original linear program starting at the BFS \\(x_i = k_i\\) for \\(1 \\le i \\le n\\) by repeatedly performing pivot steps. Exercise 4.4 This exercise provides a geometric intuition behind the auxiliary linear program. For each of the following standard linear programs, \\[\\begin{equation*} \\begin{array}{lrllll} \\mbox{maximize:} &amp; x \\\\ \\mbox{subject to:} &amp; -x &amp; \\le &amp; -1 \\\\ &amp; x &amp; \\ge &amp; 0 \\end{array} \\end{equation*}\\] \\[\\begin{equation*} \\begin{array}{lrllll} \\mbox{maximize:} &amp; x \\\\ \\mbox{subject to:} &amp; x &amp; \\le &amp; -1 \\\\ &amp; x &amp; \\ge &amp; 0 \\end{array} \\end{equation*}\\] Form the auxiliary linear program. Use \\(y\\) as the auxiliary variable. Draw the feasible region of the auxiliary linear program and label the constraints using slack variables. Solve the auxiliary linear program using geometry. 4.2 Combined tableau At the end of Phase I, we need to calculate the dictionary at the basic feasible solution \\(x_i = k_i\\) for \\(1 \\le i \\le n\\). We can avoid this recalculation by manipulating a combined tableau which contains information about both the auxiliary and the primary linear programs. Definition 4.3 The combined tableau is defined as follows: \\[\\begin{equation*} \\begin{array}{rrrrrrrrrrrr|l} a_{11} &amp; &amp; \\dots &amp; &amp; a_{1n} &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; -1 &amp;b_1\\\\ a_{21} &amp; &amp; \\dots &amp; &amp; a_{2n} &amp; &amp; &amp; 1 &amp; &amp; &amp; &amp; -1 &amp;b_2\\\\ &amp; &amp; &amp; &amp; &amp; \\vdots &amp; &amp; &amp; &amp; &amp; &amp; \\vdots &amp; \\vdots \\\\ a_{m1} &amp; &amp; \\dots &amp; &amp; a_{mn} &amp; &amp; &amp; &amp; &amp; &amp; 1 &amp; -1 &amp;b_m\\\\ \\hline c_1 &amp; &amp; \\dots &amp; &amp; c_{n} &amp; 0 &amp; &amp; 0 &amp; \\dots &amp; &amp; 0 &amp; 0 &amp; -c_0\\\\ \\hline 0 &amp; &amp; \\dots &amp; &amp; 0 &amp; 0 &amp; &amp; 0 &amp; \\dots &amp; &amp; 0 &amp; -1 &amp; 0 \\end{array} \\end{equation*}\\] The second to last row of the tableau corresponds to the objective function of the primary linear program and the last row corresponds to the objective of the auxiliary linear program. The second to last column corresponds to the variable \\(x_0\\). We use the combined tableau to first perform Phase I and then delete the auxiliary objective and the variable \\(x_0\\) and proceed on to Phase II using the rest of the tableau. Example 4.1 Consider the following linear program: \\[\\begin{equation*} \\begin{array}{rrrrrl} \\mbox{maximize:} &amp; x &amp; + &amp; y \\\\ \\mbox{subject to:} &amp; x &amp; + &amp; 2y &amp; \\le &amp; 6 \\\\ &amp; -x &amp; &amp; &amp; \\le &amp; -1 \\\\ &amp; &amp; &amp; -y &amp; \\le &amp; -2 \\\\ &amp; x &amp; , &amp; y &amp; \\ge &amp; 0. \\end{array} \\end{equation*}\\] As \\(b_2\\) and \\(b_3\\) are negative, \\((0,0)\\) is not feasible and we need to start with Phase I. We form the combined tableau: \\[\\begin{equation*} \\begin{array}{rrrrrr|r} 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 6 \\\\ -1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; -1 \\\\ 0 &amp; -1 &amp; 0 &amp; 0 &amp; 1 &amp; \\boxed{-1} &amp; -2 \\\\ \\hline 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\hline 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; 0 \\end{array} \\end{equation*}\\] The first pivot is in the column corresponding to the variable \\(x_0\\) and the row corresponding to the most negative \\(b_i\\) (which is \\(b_3 = -2\\)). This results in the following combined tableau: \\[\\begin{equation*} \\begin{array}{rrrrrr|r} 1 &amp; 3 &amp; 1 &amp; 0 &amp; -1 &amp; 0 &amp; 8 \\\\ -1 &amp; 1 &amp; 0 &amp; 1 &amp; -1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 1 &amp; 2 \\\\ \\hline 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\hline 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 2 \\end{array} \\end{equation*}\\] We then continue performing pivot steps to find the solution: \\[\\begin{equation*} \\begin{array}{rrrrrr|r} 0 &amp; 0 &amp; 1 &amp; 1 &amp; 2 &amp; -4 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 1 &amp; 2 \\\\ 1 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 1 &amp; 1 \\\\ \\hline 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; -2 &amp; -3 \\\\ \\hline 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; \\mathbf{0} \\end{array} \\end{equation*}\\] We see that the optimal value is 0 and hence the primary linear program is feasible. We then remove the auxiliary objective and coefficient to get the following tableau for the primary linear program: \\[\\begin{equation*} \\begin{array}{rrrrr|r} 0 &amp; 0 &amp; 1 &amp; \\boxed{1} &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 2 \\\\ 1 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 1 \\\\ \\hline 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; -3 \\end{array} \\end{equation*}\\] Continuing with the simplex method we get the following final tableau \\[\\begin{equation*} \\begin{array}{rrrrr|r} 0 &amp; 0 &amp; 1 &amp; 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 2 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 2 &amp; 2 \\\\ \\hline 0 &amp; 0 &amp; -1 &amp; 0 &amp; -1 &amp; -4 \\end{array} \\end{equation*}\\] The final non-basic variables are \\(w_1\\) and \\(w_3\\), the basic variables are \\(x\\), \\(y\\), \\(w_2\\) with values \\(2\\), \\(2\\), \\(1\\), respectively, and the optimal objective value is \\(4\\). The following figure shows the starting and ending basic feasible solutions for Phase II. "],["cycling.html", "Chapter 5 Cycling 5.1 Degeneracy 5.2 Blands Rule", " Chapter 5 Cycling We know how to start the simplex method and how to perform the pivot steps. Well next analyze the halting conditions for the simplex method. We will not be able to perform the pivot steps if no entering or leaving variables are found. If no entering variable is found, then the geometry tells us that there is no direction in which the objective value can be increased i.e. were at a local maxima. But because the objective function is a linear function this local maxima is also an absolute maxima and provides an optimal solution to our linear program. Algebraically, this happens when none of the \\(\\bar{c}_i\\) are positive. Proposition 5.1 If none of the objective coefficients \\(\\bar{c}_i\\) are positive in the current dictionary, then the current BFS is optimal. If no leaving variable is found, then the geometry tells us that we can keep increasing the entering variable indefinitely without leaving the feasible region. Such a linear program is called unbounded. An unbounded linear program has no optimal solution as the objective value can be made arbitrary large without leaving the feasible region. Algebraically, this happens when none of the \\(\\bar{a}_{ij}\\) are positive. Proposition 5.2 Suppose \\(\\bar{x}_j\\) is the entering variable. If none of the constants \\(\\bar{a}_{ij}\\) are positive in the current dictionary, then the linear program is unbounded. Another point of failure is in the initialization phase. Weve already seen that if the initialization phase fails then there is no feasible solution. We restate the result here for completion. Proposition 5.3 If some \\(b_i\\) is negative, and the initialization phase fails then the linear program is infeasible. Unfortunately, the above results are not sufficient to guarantee that the simplex method will always find a solution if one exists. It is possible for the simplex method to get stuck in a loop. This is called cycling. Exercise 5.1 Give an example showing that the variable that enters in one pivot step can become leave in the next. Exercise 5.2 Show that the variable that leaves in one pivot step cannot enter in the next. 5.1 Degeneracy Cycling does not always occur when running the simplex method. We saw in Section 3.1 that after the pivot step the entering and leaving variable gets updated as follows: \\[\\begin{align*} \\bar{x}_j &amp; \\mapsto \\bar{b}_i/\\bar{a}_{ij} \\\\ \\bar{w}_i &amp; \\mapsto 0. \\end{align*}\\] This increases the value of the objective function by \\(\\bar{c}_j \\bar{b}_i/\\bar{a}_{ij}\\). Because of the criterion for choosing the entering and leaving variables, the constants \\(\\bar{c}_i\\) and \\(\\bar{a}_{ij}\\) are always positive. We know that \\(\\bar{b}_i\\) this is the value of the basic variable \\(\\bar{w}_i\\) and hence must be greater than or equal to 0. If \\(\\bar{b}_i &gt; 0\\) then the objective value will increase in the pivot step and we will never return back to this BFS. Hence, the only case when cycling can occur is when \\(\\bar{b}_i = 0\\). But \\(\\bar{b}_i\\) is the value of the basic variable \\(\\bar{w}_i\\). Having more than \\(n\\) variables vanishing at a BFS is precisely the definition of degeneracy 2.3. Proposition 5.4 The simplex method can cycle if there some degenerate BFS. Example 5.1 The following slight modification of Example (1.2) is a degenerate linear program: \\[\\begin{equation*} \\begin{array}{rrrrrl} \\mbox{maximize:} &amp; 4x &amp; + &amp; 3y \\\\ \\mbox{subject to:} &amp; 3x &amp; + &amp; 6y &amp; \\le &amp; 4.5 \\\\ &amp; 2x &amp; + &amp; y &amp; \\le &amp; 1.5 \\\\ &amp; x &amp; + &amp; y &amp; \\le &amp; 1 \\\\ &amp; x &amp; , &amp; y &amp; \\ge &amp; 0. \\end{array} \\end{equation*}\\] At the optimal solution, \\((0.5, 0.5)\\) all three constraints are met. At this BFS, one of the rows is \\[\\begin{align} w_3 &amp;= 0 + 0.33 w_2 + 0.11 w_1. \\end{align}\\] Example 5.2 Consider the following degenerate linear program: \\[\\begin{equation*} \\begin{array}{rrrrrrrrrl} \\mbox{maximize:} &amp; x_1 &amp; - &amp; 2x_2 &amp; &amp; &amp; - &amp; 2x_4 \\\\ \\mbox{subject to:} &amp; 0.5 x_1 &amp; - &amp; 3.5x_2 &amp; - &amp; 2x_3 &amp; + &amp; 4 x_4 &amp; \\le &amp; 0 \\\\ &amp; 0.5 x_1 &amp; - &amp; x_2 &amp; - &amp; 0.5 x_3 &amp; + &amp; 0.5 x_4 &amp; \\le &amp; 0 \\\\ &amp; x_1 &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\le &amp; 1 \\\\ &amp; x_1 &amp; , &amp; x_2 &amp; , &amp; x_3 &amp; , &amp; x_4 &amp; \\ge &amp; 0 \\\\ \\end{array} \\end{equation*}\\] The following is a valid sequence of simplex steps: \\(x_1\\) enters and \\(w_1\\) leaves, \\(x_2\\) enters and \\(w_2\\) leaves, \\(x_3\\) enters and \\(x_1\\) leaves, \\(x_4\\) enters and \\(x_2\\) leaves, \\(w_1\\) enters and \\(x_3\\) leaves, \\(w_2\\) enters and \\(x_4\\) leaves. At the end of the \\(6^{th}\\) simplex step, we end up looping back to the origin. 5.2 Blands Rule There are various ways of dealing with cycling. The simplest such way is called Blands rule. Blands rule says that if there are multiple candidates for the entering/variable then we choose the one with the smallest index. (We assume that the decision variables have a smaller index than the slack variables.) Theorem 5.1 (Bland's rule) The simplex method always terminates provided that both the entering and the leaving variable are chosen according to Blands rule. The proof of this theorem is too complicated for this course. With this modification, for both Phase I and Phase II of the simplex method, we now have a complete algorithm for solving linear programs. Example 5.3 In Example 5.2, the sixth simplex step violates Blands rule. Both \\(x_1\\) and \\(x_4\\) can be leaving variables and we choose \\(x_4\\) whereas Blands rule requires us to choose \\(x_1\\). "],["standardization.html", "Chapter 6 Standardization 6.1 Equivalence of Linear Programs", " Chapter 6 Standardization We have an algorithm for solving a standard linear program. Well not extend it to more general linear programs, which is simply an optimization problem where the objective function is a linear program and the constraints are linear equalities or inequalities. Definition 6.1 A general linear program is an optimization problem of the following form: \\[\\begin{equation} \\begin{array}{llllllll} \\mbox{optimize: } &amp; c_1 x_1 &amp; + &amp; \\dots &amp; + &amp; c_n x_n &amp; \\\\ \\mbox{subject to: } &amp; a_{11} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{1n} x_n &amp; \\lesseqqgtr &amp; b_1 \\\\ &amp; a_{21} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{2n} x_n &amp; \\lesseqqgtr &amp; b_2 \\\\ &amp; &amp; &amp; \\vdots &amp; \\\\ &amp; a_{m1} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{mn} x_n &amp; \\lesseqqgtr b_m, \\end{array} \\tag{6.1} \\end{equation}\\] where the symbol \\(\\lesseqqgtr\\) stands for \\(\\leq\\) or \\(=\\) or \\(\\geq\\), \\(a_{ij}, b_i, c_j\\) are real numbers, and \\(x_j\\) are variables. We do not allow strict inequalities \\(&lt;\\) or \\(&gt;\\) in a linear program as linear functions do not always achieve maxima/minima on open sets even when the sets are bounded. Consider the following simple example. \\[\\begin{align*} \\mbox{maximize:} &amp;&amp; x \\\\ \\mbox{subject to:} &amp;&amp; x &amp;&lt; 1 \\\\ &amp;&amp; x &amp;\\ge 0 \\end{align*}\\] On the feasible set \\([0, 1)\\), the function \\(x\\) never attains absolute maxima. Changing the inequality \\(&lt;\\) to \\(\\leq\\) gives us an optimal feasible solution \\(x = 1\\). 6.1 Equivalence of Linear Programs Any linear program can be changed to a maximization problem as minimizing a function is the same as maximizing its negation. From now on, well assume that the goal of our linear programs is to maximize the objective function. Definition 6.2 Two (maximizing) linear programs LP and LP are said to be equivalent if for any feasible solution \\((x_1, \\dots, x_n)\\) to LP, there exists a feasible solution \\((x&#39;_1, \\dots, x&#39;_{n&#39;})\\) to LP with the same objective value and for any feasible solution \\((y&#39;_1, \\dots, y&#39;_n)\\) to LP, there exists a feasible solution \\((y_1, \\dots, y_{n})\\) to LP with the same objective value . Thus solving LP is equivalent to LP. Remark.   1. LP and LP can have a different number of decision variables i.e. we do not require \\(n = n&#39;.\\) 2. There need not be a one-to-one correspondence between the feasible sets of LP and LP i.e. for a feasible solution to LP there could be multiple feasible solutions with the same objective value. Similarly, in the other direction. Remark. Even though equivalence of linear programs only requires the existence of an abstract correspondence between the feasible sets of LP and LP, in practice, one constructs linear transformations \\(T: \\mathbb{R}^n \\to \\mathbb{R}^{n&#39;}\\) and \\(S: \\mathbb{R}^{n&#39;} \\to \\mathbb{R}^{n}\\) which map the feasible set of LP to LP and the feasible set of LP to LP, respectively. These linear transformations need not be inverses of each other, or even isomorphisms. They only need to preserve the objective values. Example 6.1 The following two linear programs are equivalent to each other \\[\\begin{align*} \\begin{aligned} \\mbox{maximize: } &amp;&amp; x + y &amp; \\\\ \\mbox{subject to: } &amp;&amp; 0 \\le x &amp;\\le 1 \\\\ &amp;&amp; 0 \\le y &amp;\\le 1 \\end{aligned} &amp;&amp; \\begin{aligned} \\mbox{maximize: } &amp;&amp; z &amp; \\\\ \\mbox{subject to: } &amp;&amp; 0 \\le z &amp;\\le 1 \\end{aligned} \\end{align*}\\] via the transformations \\(T(x, y) = x + y\\) and \\(S(z) = (z/2, z/2)\\). Theorem 6.1 Every linear program is equivalent to a linear program in the standard form. Proof. The proof is by providing an explicit standardization algorithm. Consider the linear program in (6.1), where were assuming that the goal is to maximize the objective function. If it is in the standard form, then were done. If not, then there must be a finite number of errors of the following types: A linear constraint is a lower bound and has the form \\[a_{i1} x_1 + \\dots + a_{in} x_n \\geq b_i.\\] A linear constraint is an equality and has the form \\[a_{i1} x_1 + \\dots + a_{in} x_n = b_i.\\] A variable \\(x_j\\) has a negativity constraint \\(x_j \\leq 0\\). A variable \\(x_j\\) is free i.e. it is missing a sign constraint. We fix these errors one at a time, while making sure that no new errors are introduced, thereby guaranteeing termination of the algorithm. We replace the constraint with \\[ -a_{i1} x_1 + \\dots + -a_{in} x_n \\leq -b_i. \\] We replace the constraint with two constraints \\[\\begin{align*} a_{i1} x_1 + \\dots + a_{in} x_n &amp;\\leq b_i \\\\ -a_{i1} x_1 - \\dots - a_{in} x_n &amp;\\leq -b_i, \\end{align*}\\] We let \\(y_j = -x_j\\) and create a new linear program using the variables \\(x_1\\), \\(\\dots\\), \\(x_{j-1}\\), \\(y_j\\), \\(x_{j+1}\\), \\(\\dots\\), \\(x_n\\) by replacing \\(x_j\\) with \\(-y_j\\) everywhere. We let \\(x_j = y_j - z_j\\) for two new decision variables \\(y_j\\) and \\(z_j\\) with \\(y_j, z_j \\geq 0\\) and create a new linear program using the variables \\(x_1\\), \\(\\dots\\), \\(x_{j-1}\\), \\(y_j\\), \\(z_j\\), \\(x_{j+1}\\), \\(\\dots\\), \\(x_n\\) by replacing \\(x_j\\) with \\(y_j - z_j\\) everywhere. We can do this because any real number can written as a difference of two positive real numbers. Exercise 6.1 Prove that the algorithm in the proof of Theorem 6.1 produces a linear program that is equivalent to the original linear program. "],["dual-linear-program.html", "Chapter 7 Dual Linear Program 7.1 General Linear Program", " Chapter 7 Dual Linear Program So far weve constructed a method for solving linear programs. Our next goal is to understand the dependence of the solution on the various constants in the program. We will do this using duality theory. We start by introducing the matrix notation. Well let \\(x\\) denote the vector of decision variables, \\(b\\) the vector of upper bounds, \\(c\\) the vector of objective coefficients, and \\(A\\) the matrix of constraints in the standard linear program (2.1). \\[\\begin{align*} x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, \\quad b = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix}, \\quad c = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}, \\quad A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\dots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\dots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\dots &amp; a_{mn} \\end{bmatrix}. \\end{align*}\\] The standard linear program (2.1) can be written as follows: \\[\\begin{equation*} \\begin{array}{lrll} \\mbox{maximize: } &amp; c_0 + c^T x \\\\ \\mbox{subject to: } &amp; A x &amp; \\leq &amp; b \\\\ &amp; x &amp; \\geq &amp; 0. \\end{array} \\end{equation*}\\] Definition 7.1 The dual of this linear program is defined as the following linear program: \\[\\begin{equation} \\begin{array}{lrll} \\mbox{minimize: } &amp; c_0 + b^T y \\\\ \\mbox{subject to: } &amp; A^T y &amp; \\geq &amp; c \\\\ &amp; y &amp; \\geq &amp; 0, \\end{array} \\tag{7.1} \\end{equation}\\] where \\(y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}\\) is the vector of dual decision variables. The original linear program is called the primal. The dual decision variables correspond to the constraints of the original linear program. By explicitly computation, we can easily prove that the following theorem. Theorem 7.1 The the dual of the (standardized) dual is equivalent to the primal. Hence, we think of linear programs as occurring in primal-dual pairs. Every linear program has a dual and it is itself the dual of the dual. The dual decision variables correspond to the constraints of the original linear program and the primal decision variables correspond to the constraints of the dual. The meaning of the dual decision variables will become clear when we study shadow prices. Example 7.1 The dual of (1.2) is: \\[\\begin{equation} \\begin{array}{lrrrrrrrrrr} \\mbox{minimize: } &amp; 3.6 y_1 &amp; + &amp; 1.5 y_2 &amp; + &amp; y_3 \\\\ \\mbox{subject to: } &amp; 3y_1 &amp; + &amp; 2 y_2 &amp; + &amp; y_3 &amp; \\geq &amp; 4 \\\\ &amp; 6y_1 &amp; + &amp; y_2 &amp; + &amp; y_3 &amp; \\geq &amp; 3 \\\\ &amp; y_1 &amp; , &amp; y_2 &amp; , &amp; y_3 &amp; \\geq &amp; 0. \\end{array} \\tag{7.2} \\end{equation}\\] The variable \\(y_1\\) corresponds to maturity, the variable \\(y_2\\) corresponds to risk, and the variable \\(y_3\\) corresponds to percentage. 7.1 General Linear Program We saw how to standardized general linear programs in Chapter 6. We need to fix the objective function, constraints, and signs of decision variables. To find the dual of a general linear program, we can first standardize it and then form the dual. However, doing so changes the constants. It is possible to revert the standardization in the resulting dual to get the same constants as the primal. Suppose one of the linear constraint is a lower bound and has the form \\[a_{i1} x_1 + \\dots + a_{in} x_n \\geq b_i.\\] To fix this, we multiply the constraint by \\(-1\\) to get \\[-a_{i1} x_1 + \\dots + - a_{in} x_n \\leq -b_i.\\] If we then form the dual, the coefficients of the dual variable \\(y_i\\) will be \\(-a_{ij}\\) instead of \\(a_{ij}\\). We can replace \\(y_i\\) with a variable \\(y_i&#39; = -y_i\\) and then the resulting linear program will have the same constants as the primal but now we have \\(y_i&#39; \\le 0\\). Notice that this is the third type of error in a general linear program. Suppose one of the linear constraint is a lower bound and has the form \\[a_{i1} x_1 + \\dots + a_{in} x_n = b_i.\\] To fix this, we replace the constraint with the two constraints \\[\\begin{align*} a_{i1} x_1 + \\dots + a_{in} x_n &amp;\\leq b_i, \\\\ -a_{i1} x_1 - \\dots - a_{in} x_n &amp;\\leq -b_i. \\end{align*}\\] If we then form the dual, we will have two dual variables \\(y_i&#39;\\) and \\(y_i&#39;&#39;\\) with coefficients \\(a_{ij}\\) and \\(-a_{ij}\\) respectively. We can introduce a new variable, \\(y_i = y_i&#39; - y_i&#39;&#39;\\) and then the resulting linear program will have the same constants as the primal but now we have \\(y_i\\) is free. Notice that this is the fourth type of error in a general linear program. Because the dual of the dual is the primal, we do not need to analyze the third and fourth type of error separately. The follow table describes the modifications needed for forming the dual of a general linear program. Primal Dual \\(a_{i1} x_1 + \\dots + a_{in} x_n \\le b_i\\) \\(y_i \\ge 0\\) \\(a_{i1} x_1 + \\dots + a_{in} x_n \\ge b_i\\) \\(y_i \\le 0\\) \\(a_{i1} x_1 + \\dots + a_{in} x_n = b_i\\) \\(y_i\\) is free \\(x_j \\ge 0\\) \\(a_{1j}y_1 + \\dots + a_{mj} y_m \\ge c_j\\) \\(x_j \\le 0\\) \\(a_{1j}y_1 + \\dots + a_{mj} y_m \\le c_j\\) \\(x_j\\) is free \\(a_{1j}y_1 + \\dots + a_{mj} y_m = c_j\\) "],["weak-and-strong-duality.html", "Chapter 8 Weak and Strong Duality 8.1 Weak Duality 8.2 Strong Duality 8.3 Complimentary slackness", " Chapter 8 Weak and Strong Duality Well now prove several theorems relating the optimal objective values of the primal-dual pairs. Consider the standard linear program (primal) (2.1) and its dual (7.1). We say that a vector \\(x = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\) is primal feasible if it is in the feasible region of the primal and a vector \\(y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_m \\end{bmatrix}\\) is dual feasible if it is in the feasible region of the dual. 8.1 Weak Duality Theorem 8.1 (Weak duality) Suppose \\(x\\) is primal feasible and \\(y\\) is dual feasible. Then the primal objective value at \\(x\\) is less than or equal to the dual objective value at \\(y\\). Proof. The proof relies on analyzing the term \\(y^T A x\\) and follows by looking at the following sequence of inequalities: \\[\\begin{align*} b^T y &amp; = y^T b \\\\ &amp; \\ge y^T (Ax) &amp;&amp; \\mbox{ as } Ax \\leq b \\mbox{ and } y \\geq 0 \\\\ &amp; = (y^T A x)^T \\\\ &amp; = x^T A^T y \\\\ &amp; \\ge x^T c &amp;&amp; \\mbox{ as } A^Ty \\geq c \\mbox{ and } x \\geq 0 \\\\ &amp; = c^T x. \\end{align*}\\] We get several immediate corollaries from weak duality. Corollary 8.1 If the primal is unbounded, then the dual is infeasible. Corollary 8.2 If the dual is unbounded, then the primal is infeasible. Corollary 8.3 If both the primal and dual have optimal solutions, then the optimal value of the primal is less than or equal to the optimal value of the dual. We cannot say anything about the dual in the case when the primal is infeasible. Similarly, we cannot conclude anything about the existence of an optimal value of the dual in the case when the primal has an optimal solution. 8.2 Strong Duality The tableau of the primal problem (2.1) is as follows: \\[\\begin{equation} \\begin{array}{ll|r} c^T &amp; 0 &amp; c_0 \\\\ \\hline A &amp; I_m &amp; b \\end{array} \\tag{8.1} \\end{equation}\\] We can standardize the dual problem (7.1) and form its tableau: \\[\\begin{equation} \\begin{array}{ll|r} -b^T &amp; 0 &amp; -c_0 \\\\ \\hline -A^T &amp; I_n &amp; -c \\end{array} \\tag{8.2} \\end{equation}\\] We will call such tableaux duals of each other. More generally, well say that two tableaux (of appropriate dimensions) are duals of each other if after rearranging the pivot columns, if necessary, theyre of the above forms. One can show the following theorem by explicit computation: Lemma 8.1 Consider the two dual tableaux (8.1) and (8.2) . If we pivot the first tableau about the \\(i^{th}\\) row and \\(j^{th}\\) column of \\(A\\) and the second tableau about the \\(i^{th}\\) column and \\(j^{th}\\) row of \\(-A^T\\), then the resulting tableaux remain duals of each other. Lemma 8.2 If the tableau (8.1) corresponds to an optimal solution of the primal then the tableau (8.2) corresponds to an optimal solution of the dual. Proof. The tableau (8.1) corresponds to an optimal solution of the primal precisely when (primal optimality) \\(c^T \\le 0\\) as in this case no entering variable can be found for the primal, and (primal feasibility) \\(b \\ge 0\\). These conditions translate to (dual optimality) \\(-b^T \\le 0\\) as in this case no entering variable can be found for the dual, and (dual feasibility) \\(-c \\ge 0\\). Using the above two lemmas, and by explicitly running the simplex method we get the following result: Theorem 8.2 (Strong duality) If the primal has an optimal solution then so does the dual. Moreover, they have the same optimal values. Proof. At the optimal solution for the primal, we have a set of basic and non-basic variables. We can perform a sequence of pivot operations to get this tableau from the initial tableau. We then perform the corresponding pivots on the dual tableau. By Lemma 8.1 the resulting tableau will be dual to the primal tableau at the optimal solution. By Lemma 8.2 the dual is also optimal and has the same objective value. We can use the two duality theorems to come up with a fast way to verify optimality. Theorem 8.3 (Certificate of optimality) \\(x\\) is an optimal solution for the primal and \\(y\\) is an optimal solution for the dual if and only if 1. \\(x\\) is primal-feasible, 2. \\(y\\) is dual-feasible, 3. \\(c^T x = b^T y\\) i.e. the primal objective value at \\(x\\) is equal to the dual objective value at \\(y\\). Proof. ( \\(\\Rightarrow\\) ) If \\(x\\) and \\(y\\) are optimal solutions, then they are feasible by definition. By strong duality (Theorem 8.2) they must the same objective value. ( \\(\\Leftarrow\\) ) If \\(x\\) and \\(y\\) are feasible solutions then by weak duality (Theorem 8.1) the dual objective values provide an upper bound on the primal objective value. Because this upper bound is attained at \\(x\\), \\(x\\) must be an optimal solution of the primal. Similarly, for \\(y\\). 8.3 Complimentary slackness There is another closely related method for verifying the correctness of solution using primal and dual slack variables. Denote by \\(w = \\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_m \\end{bmatrix}\\) the primal slack variables and by \\(z = \\begin{bmatrix} v_1 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\) the dual slack variables. More explicitly, \\[\\begin{align*} w &amp;= b - A x \\\\ z &amp;= -c + A^T y. \\end{align*}\\] We use this convention for \\(z\\) as then at a dual feasible solution \\(z \\ge 0\\). Theorem 8.4 (Complementary slackness) Suppose \\(x\\) is primal feasible and \\(y\\) is dual feasible. Then \\(x\\) and \\(y\\) are optimal if and only if for all \\(1 \\le j \\le n\\), \\(x_j z_j = 0\\), and for all \\(1 \\le i \\le m\\), \\(y_i w_i = 0\\). Proof. The proof is in two steps. We first show a weaker statement about the vanishing of two scalars and then show that the vanishing of these scalars implies complementary slackness. Claim: \\(x\\) and \\(y\\) are optimal solutions if and only if \\(x^T z = 0\\) and \\(y^T w = 0\\). We start by rewriting the slack variable: \\[\\begin{align*} &amp;&amp; x^T z = 0 &amp;&amp; \\mbox{ and } &amp;&amp; y^T w = 0 \\\\ \\Leftrightarrow &amp;&amp; x^T (-c + A^T y) = 0 &amp;&amp; \\mbox{ and } &amp;&amp; y^T (b - A x) = 0 \\\\ \\Leftrightarrow &amp;&amp; x^T c = x^T A^T y &amp;&amp; \\mbox{ and } &amp;&amp; y^T b = y^T A x \\\\ \\Leftrightarrow &amp;&amp; c^T x = y^T A x &amp;&amp; \\mbox{ and } &amp;&amp; b^T y = y^T A x \\end{align*}\\] Thus we are reduced to showing that \\(x\\) and \\(y\\) are optimal solutions if and only if \\(c^T x = y^T A x\\) and \\(b^T y = y^T A x\\). ( \\(\\Leftarrow\\) ) As \\(c^T x = y^T A x = b^T y\\), and \\(x\\) and \\(y\\) are given to be feasible, \\(x\\) and \\(y\\) are optimal by Theorem 8.3. ( \\(\\Rightarrow\\) ) By the proof of weak duality 8.1, we know that as \\(x\\) and \\(y\\) are feasible, \\[c^T x \\le y^T A x \\le b^T y.\\] By strong duality, as \\(x\\) and \\(y\\) are optimal \\[c^T x = b^T y.\\] The only way the two can be simultaneously true is if \\(c^T x = y^T A x\\) and \\(b^T y = y^T A x\\). Claim: \\(x^T z = 0\\) and \\(y^T w = 0\\) if and only if for all \\(1 \\le j \\le n\\), \\(x_j z_j = 0\\), and for all \\(1 \\le i \\le m\\), \\(y_i w_i = 0\\). This follows from the fact that at a feasible solution \\(x, y, w, z \\ge 0\\). Remark. Complementary slackness even holds for dual of general linear programs. The definition of slack variables as above \\[\\begin{align*} w &amp;= b - A x \\\\ z &amp;= -c + A^T y. \\end{align*}\\] still ensures that we will have \\(x_j w_j \\ge 0\\) for all \\(1 \\le j \\le n\\) and \\(y_i z_i \\ge 0\\) for all \\(1 \\le i \\le m\\) and hence the above proof will go through. If the optimal solution of the primal is at a non-degenerate BFS, then complementary slackness can be used to find the optimal solution to the dual without having to run the simplex method on it. At a non-degenerate BFS, the basic variables are non-zero. By complementary slackness, the corresponding dual variables must be 0. This gives us a system of \\(m\\) variables in \\(m\\) equations which can be solved to find the dual optimal solution. Example 8.1 Consider the linear program (1.2) whose dual is (7.2). At the optimal solution \\(x_1 = 0.6\\), \\(x_2 = 0.3\\), \\(w_1 = 0\\), \\(w_2 = 0\\), and \\(w_3 = 0.1\\). By complementary slackness, we must have \\(z_1 = 0\\), \\(z_2 = 0\\), and \\(y_3 = 0\\) at the dual optimal solution. Plugging these in (1.2), we get a system of equations \\[\\begin{align*} 3y_1 + 2y_2 &amp;= 4 \\\\ 6y_1 + y_2 &amp;= 3 \\end{align*}\\] whose solutions are \\(y_1 = 2/9\\) and \\(y_2 = 5/3\\). One can check that this is indeed dual optimal by comparing the dual objective at this point to the primal objective: \\[\\begin{align*} 3.6 (2/9) + 1.5 (5/3) + 0 (1) = 3.3 = 4 (0.6) + 3 (0.3). \\end{align*}\\] Exercise 8.1 We can use complementary slackness and certificate of optimality to show that the optimal solution of a standard linear program can never be attained in the interior of the feasible region. Consider the standard linear program (2.1) and assume that \\(b \\ge 0\\) and \\(c \\neq \\vec{0}\\). Let \\(x^*\\) be a point in the . Suppose \\(x^*\\) is an optimal solution to the primal. By strong duality, we know that a dual-optimal solution \\(y^*\\) exists. Well show that this leads to a contradiction. What can you say about the values of the decision and slack variables at \\(x^*\\)? What can you say about the values of the dual decision variables at \\(y^*\\) using complementary slackness? We now consider two cases, depending on the sign of \\(c\\), and in each case show that \\(y^*\\) cannot be optimal. Suppose that \\(c_j &gt; 0\\) for some \\(0 \\le j \\le n\\). Explain why \\(y^*\\) cannot be dual-feasible (and hence optimal). Suppose \\(c \\le 0\\). What can you say about the objective values at \\(x^*\\) and \\(y^*\\)? Explain why \\(y^*\\) cannot be dual-optimal. "],["sensitivity-analysis.html", "Chapter 9 Sensitivity Analysis 9.1 Dictionaries Revisited 9.2 Range of Optimality - Constraints 9.3 Shadow Prices 9.4 Sensitivity analysis - Objective", " Chapter 9 Sensitivity Analysis Linear programs are used to model real world problems. Such models are at best approximate and at worst inaccurate. As such, it is important to understand the sensitivity of our solution to changes in the model. This is broadly called sensitivity analysis. We will focus on understanding the dependence of the optimal objective value of the standard linear program (2.1) on the constants \\(b_i\\) and \\(c_j\\). Throughout this chapter, well assume that our linear programs have an optimal solution. 9.1 Dictionaries Revisited We will start by finding a succinct way to describe the dictionary at the optimal solution. Recall that the decision and slack variables are related to each other by the Equation (3.5) which can be written as: \\[\\begin{equation} \\begin{bmatrix} A &amp; I_m \\end{bmatrix} \\begin{bmatrix} x \\\\ w \\end{bmatrix} = b. \\tag{9.1} \\end{equation}\\] Let \\(\\widehat{A} := \\begin{bmatrix} A &amp; I_m \\end{bmatrix}\\). Well decompose \\(\\widehat{A}\\) using the basic and non-basic variables. Then let \\(\\mathcal{B}\\) be the matrix formed by combining the columns of \\(\\widehat{A}\\) corresponding to the basic variables at the optimal BFS and let \\(\\mathcal{N}\\) be the matrix formed by combining the columns of \\(\\widehat{A}\\) corresponding to the non-basic variables at the optimal BFS. Let \\(x_{\\mathcal{B}}\\) be the vector of basic variables and \\(x_{\\mathcal{N}}\\) be the vector of non-basic variables. By rearranging the columns of \\(\\widehat{A}\\) if necessary, we can rewrite (9.1) as \\[\\begin{align*} &amp;&amp; \\begin{bmatrix} \\mathcal{B} &amp; \\mathcal{N} \\end{bmatrix} \\begin{bmatrix} x_{\\mathcal{B}} \\\\ x_{\\mathcal{N}} \\end{bmatrix} &amp;= b,\\\\ \\implies &amp;&amp; \\mathcal{B} x_{\\mathcal{B}} + \\mathcal{N} x_{\\mathcal{N}} &amp;= b, \\\\ \\implies &amp;&amp; \\mathcal{B} x_{\\mathcal{B}} &amp;= b - \\mathcal{N} x_{\\mathcal{N}}. \\end{align*}\\] When we execute the simplex method, the matrix \\(\\mathcal{B}\\) gets reduced to an \\(n \\times n\\) matrix with \\(n\\) pivots. Hence, it is invertible. \\[\\begin{equation} \\implies x_{\\mathcal{B}} = \\mathcal{B}^{-1} b - \\mathcal{B}^{-1} \\mathcal{N} x_{\\mathcal{N}}. \\tag{9.2} \\end{equation}\\] This is nothing but the dictionary at the optimal BFS. Example 9.1 Consider Example (1.2) again. At the optimal solution \\(w_1\\) and \\(w_2\\) are non-basic and have the value 0, and \\(x\\), \\(y\\), and \\(w_3\\) are basic with values \\(0.3\\), \\(0.6\\), and \\(0.9\\), respectively. Using the above notation, we have \\[\\begin{align*} \\mathcal{B} = \\begin{bmatrix} 3 &amp; 6 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}, x_{\\mathcal{B}} = \\begin{bmatrix} x \\\\ y \\\\ w_3 \\end{bmatrix}, \\\\ \\mathcal{N} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix}, x_{\\mathcal{N}} = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}. \\end{align*}\\] Using Equation (9.2) the dictionary at the optimal solution becomes \\[\\begin{align*} \\begin{bmatrix} x \\\\ y \\\\ w_3 \\end{bmatrix} &amp;= \\begin{bmatrix} 3 &amp; 6 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}^{-1} \\begin{bmatrix} 3.6 \\\\ 1.5 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 3 &amp; 6 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}^{-1} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} -1/9 &amp; 2/3 &amp; 0 \\\\ 2/9 &amp; -1/3 &amp; 0 \\\\ -1/9 &amp; -1/3 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 3.6 \\\\ 1.5 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} -1/9 &amp; 2/3 &amp; 0 \\\\ 2/9 &amp; -1/3 &amp; 0 \\\\ -1/9 &amp; -1/3 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} 0.6 \\\\ 0.3 \\\\ 0.1 \\end{bmatrix} - \\begin{bmatrix} -1/9 &amp; 2/3 \\\\ 2/9 &amp; -1/3 \\\\ -1/9 &amp; -1/3 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}. \\end{align*}\\] This is precisely the dictionary (3.3) at the optimal solution. Because we set the non-basic variables \\(x_{\\mathcal{N}}\\) to 0 at any vertex, and in particular, at the the optimal solution, using the Equation (9.2) we get the following useful result: Lemma 9.1 Using the above notation, \\(\\mathcal{B}^{-1}b\\) is the value of the basic variables \\(x^*_{\\mathcal{B}}\\) at the optimal solution. 9.2 Range of Optimality - Constraints We want to analyze the change in the optimal solution as we change the constraint upper bounds \\(b_i\\). It is likely that by changing \\(b_i\\) we change the optimal solution. However, in a good model, this change should not be abrupt. This can be achieved by requiring the set of basic and non-basic variables to remain unchanged. In this case, the equation (9.2) will still be the equation describing the dictionary at the optimal solution and the change in \\(b_i\\) will result in a differentiable (in fact, linear) change in \\(x_{\\mathcal{B}}\\). Example 9.2 Suppose we vary \\(b_3 = 1\\) in Example (1.2). One can check that at the optimal solution \\(w_1\\) and \\(w_2\\) are non-basic as long as \\(b_3 &gt; 0.9\\). Thus we can say that out model is a good model as long as the error in \\(b_3\\) is less than \\(0.1\\). Suppose we change \\(b_i\\) to \\(b_i + \\delta\\), where \\(\\delta\\) is a real number, and leave all the other constants unchanged. This is equivalent to changing \\(b\\) to \\(b + \\delta e_i\\) where \\(e_i\\) is the \\(i^{th}\\) standard basis vector of \\(\\mathbb{R}^m\\). This changes equation (9.2) to \\[\\begin{align*} x_{\\mathcal{B}} &amp;= \\mathcal{B}^{-1} b + \\delta \\mathcal{B}^{-1} e_i - \\mathcal{B}^{-1} \\mathcal{N} x_{\\mathcal{N}} \\\\ &amp;= \\mathcal{B}^{-1} b + \\delta (\\mathcal{B}^{-1})_{\\_i} - \\mathcal{B}^{-1} \\mathcal{N} x_{\\mathcal{N}}. \\end{align*}\\] where \\((\\mathcal{B}^{-1})_{\\_i}\\) denotes the \\(i^{th}\\) column of \\(\\mathcal{B}^{-1}\\). Note that the coefficients of \\(x_{\\mathcal{N}}\\) remain unchanged. So, for this dictionary to stay optimal we only need the constants to remain non-negative i.e. \\[\\begin{equation} \\mathcal{B}^{-1} b + \\delta (\\mathcal{B}^{-1})_{\\_i} \\ge 0. \\tag{9.3} \\end{equation}\\] Proposition 9.1 The range of optimality for \\(b_i\\) is the interval \\([b_i + \\delta_-, b_i + \\delta_+]\\) such that \\(\\mathcal{B}^{-1} b + \\delta (\\mathcal{B}^{-1})_{\\_i} \\ge 0\\) for all \\(\\delta \\in [\\delta_-, \\delta_+]\\). In practice, Equation (9.3) gives us \\(m\\) inequalities which need to be simultaneously satisfied. These give us candidate values for \\(\\delta\\) some of which are positive and some of which are negative. We then choose \\(\\delta_+\\) to be the smallest positive value and \\(\\delta_-\\) to be the largest negative value. If \\(\\delta_+\\) does not exist the upper bound is \\(\\infty\\) and if \\(\\delta_-\\) does not exist the lower bound is \\(-\\infty\\). If either \\(\\delta_+\\) or \\(\\delta_-\\) is 0 then the linear program is degenerate. In this case, our program is very sensitive to perturbations in \\(b_i\\). Example 9.3 Let us find the range of optimality for \\(b_1 = 3.6\\), \\(b_2=1.5\\), and \\(b_3 = 1\\) in (1.2) using our calculations in Example 9.1. We know that \\[\\begin{align*} \\mathcal{B}^{-1} = \\begin{bmatrix} -1/9 &amp; 2/3 &amp; 0 \\\\ 2/9 &amp; -1/3 &amp; 0 \\\\ -1/9 &amp; -1/3 &amp; 1 \\end{bmatrix}. \\end{align*}\\] Using \\(i = 1\\) and Lemma 9.1 in Equation (9.3) we get \\[\\begin{align*} \\begin{bmatrix} 0.6 \\\\ 0.3 \\\\ 0.1 \\end{bmatrix} + \\delta \\begin{bmatrix} -1/9 \\\\ 2/9 \\\\ -1/9 \\end{bmatrix} &gt; 0 \\end{align*}\\] which gives us the inequalities \\[\\begin{align*} \\begin{array}{lrlrrll} 0.6 + \\delta (-1/9) &amp;\\ge &amp; 0 &amp; \\implies &amp; \\delta &amp;\\le &amp; 0.6 (9) = 5.4 \\\\ 0.3 + \\delta (2/9) &amp;\\ge &amp; 0 &amp; \\implies &amp; \\delta &amp;\\ge &amp; -0.3 (9/2) = -1.35 \\\\ 0.1 + \\delta (-1/9) &amp;\\ge &amp; 0 &amp; \\implies &amp; \\delta &amp; \\le &amp; 0.1 (9) = 0.9. \\end{array} \\end{align*}\\] So, \\(\\delta_- = -1.35\\) and \\(\\delta_+ = \\min(5.4, 0.9) = 0.9\\) and the range of optimality for \\(b_1\\) is \\([3.6 - 1.35, 3.6 + 0.9] = [2.25, 4.5]\\). Using \\(i = 2\\) and Lemma 9.1 in Equation (9.3) we get \\[\\begin{align*} \\begin{bmatrix} 0.6 \\\\ 0.3 \\\\ 0.1 \\end{bmatrix} + \\delta \\begin{bmatrix} 2/3 \\\\ -1/3 \\\\ -1/3 \\end{bmatrix} &gt; 0 \\end{align*}\\] which gives us the inequalities \\[\\begin{align*} \\begin{array}{lrlrrll} 0.6 + \\delta (2/3) &amp;\\ge&amp; 0 &amp; \\implies &amp; \\delta &amp;\\ge&amp; - 0.6 (3/2) = -0.9 \\\\ 0.3 + \\delta (-1/3) &amp;\\ge &amp;0 &amp; \\implies &amp; \\delta &amp;\\le &amp;0.3 (3) = 0.9 \\\\ 0.1 + \\delta (-1/3) &amp;\\ge &amp;0 &amp; \\implies &amp; \\delta &amp; \\le&amp; 0.3 (1) = 0.3. \\end{array} \\end{align*}\\] So, \\(\\delta_- = -0.9\\) and \\(\\delta_+ = \\min(0.3, 0.9) = 0.3\\) and the range of optimality for \\(b_2\\) is \\([1.5 - 0.9, 1.5 + 0.3] = [0.6, 1.8]\\). Using \\(i = 3\\) and Lemma 9.1 in Equation (9.3) we get \\[\\begin{align*} \\begin{bmatrix} 0.6 \\\\ 0.3 \\\\ 0.1 \\end{bmatrix} + \\delta \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} &gt; 0 \\end{align*}\\] which gives us \\(\\delta \\ge -0.1\\) and so the range of optimality for \\(b_3\\) is \\([1 - 0.1, \\infty) = [0.9, \\infty)\\). The following figures show the optimal solutions at the extreme ends of the range of optimality of \\(b_1\\). 9.3 Shadow Prices Assume now that neither of \\(\\delta_+\\) or \\(\\delta_-\\) is zero. We can use Lemma 9.1 to find the rate of change of the optimal solution with respect to \\(b_i\\). Call the objective function \\(\\mathbb{O} = c^{T} x\\). We think of \\(\\mathbb{O}\\) as being a function of \\(b_i\\), \\(c_j\\), and \\(a_{ij}.\\) Using Lemma 9.1 we get \\[\\begin{align*} \\dfrac{\\partial x_{\\mathcal{B}_j}}{\\partial b_i} &amp;= j^{th} \\mbox{ row of } \\dfrac{\\partial \\mathcal{B}^{-1}b}{\\partial b_i} \\\\ &amp;= (\\mathcal{B}^{-1})_{ji} \\\\ \\end{align*}\\] where \\(x_{\\mathcal{B}_j}\\) denotes the \\(j^{th}\\) basic variable at the optimal solution. And \\[\\begin{align*} \\dfrac{\\partial x_{\\mathcal{N}}}{\\partial b_i} &amp;= 0 \\end{align*}\\] as the non-basic variables remain 0 when we perturb \\(b_i\\) within the range of optimality. Using these we can find the rate of change of the optimal solution \\(\\mathbb{O}\\) with respect to \\(b_i\\). We first start by re-indexing the variables and objective coefficients using the basic and non-basic variables. \\[\\begin{align*} \\mathbb{O} &amp;= c^T x \\\\ &amp;= c^T_{\\mathcal{B}} x_{\\mathcal{B}} + c^T_{\\mathcal{N}} x_{\\mathcal{N}} \\\\ \\implies \\dfrac{\\partial \\mathbb{O}}{\\partial b_i} &amp;= c^T_{\\mathcal{B}} \\dfrac{\\partial x_{\\mathcal{B}}}{\\partial b_i} + c^T_{\\mathcal{N}} \\dfrac{\\partial x_{\\mathcal{N}}}{\\partial b_i} \\\\ &amp;= c^T_{\\mathcal{B}} (\\mathcal{B}^{-1})_{\\_i} \\end{align*}\\] By strong duality, we know that the primal objective value equals the dual objective value i.e.  \\[\\begin{align*} \\mathbb{O} &amp;= b_1 y_1 + \\cdots + b_m y_m \\end{align*}\\] So, \\(\\partial \\mathbb{O}/\\partial b_i = y_i\\). Because of this result, \\(y_i\\) is also called the shadow price or the marginal cost of the \\(i^{th}\\) constraint. Using the rate of change calculation above, we get \\[\\begin{align*} y_i = c^T_{\\mathcal{B}} (\\mathcal{B}^{-1})_{\\_i}. \\end{align*}\\] We can combine all the above coordinates into a single vector to get the following result. Theorem 9.1 For a non-degenerate linear program, the dual optimal solution is given by \\((\\mathcal{B}^{-1})^Tc_{\\mathcal{B}}\\). This theorem provides yet another method of finding the dual optimal solution without having to solve the dual linear program. Example 9.4 For the linear program (1.2), the objective function is \\[\\begin{align*} \\mathbb{O} &amp; = 4 x + 3y \\\\ &amp; = 4x + 3y + 0 w_3 + 0 w_1 + 0 w_2 \\end{align*}\\] So, \\(c_{\\mathcal{B}} = \\begin{bmatrix}4 \\\\ 3 \\\\ 0 \\end{bmatrix}\\) and \\(c_{\\mathcal{N}} = \\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}\\). Using the value of \\(\\mathcal{B}^{-1}\\) calculated above, we get \\[\\begin{align*} y &amp; = (\\mathcal{B}^{-1})^Tc_{\\mathcal{B}} \\\\ &amp; = \\begin{bmatrix} -1/9 &amp; 2/9 &amp; -1/9 \\\\ 2/3 &amp; -1/3 &amp; -1/3 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 4 \\\\ 3 \\\\ 0 \\end{bmatrix}\\\\ &amp; = \\begin{bmatrix} 2/9 \\\\ 5/3 \\\\ 0 \\end{bmatrix}. \\end{align*}\\] To check that this is indeed dual-optimal, we calculate the dual-objective value at this solution \\[\\begin{align*} b^T y &amp; = 3.6 (2/9) + 1.5 (5/3) + 0 (1) \\\\ &amp;= 3.3 \\end{align*}\\] which equals the optimal objective value of the primal. One can check that this solution is also dual-feasible and hence is the dual-optimal solution by Certificate of Optimality (Theorem 8.3). 9.4 Sensitivity analysis - Objective We can ask the same questions about the change in the objective coefficients - how far can we change the objective coefficient \\(c_j\\) without changing the set of basic and non-basic variables at the optimal solution. Note that in this case we are not changing the constraints and therefore the feasible region. So, this is the same as asking - how far can we change the objective coefficient \\(c_j\\) without changing the optimal solution. We can redo the entire analysis for the objective coefficients from scratch. However, we can also notice that performing sensitivity analysis on the objective coefficients of the primal is the same as performing sensitivity analysis on the constraints of the dual. Consider the standardized dual \\[\\begin{equation*} \\begin{array}{lrll} \\mbox{maximize: } &amp; -c_0 -b^T y \\\\ \\mbox{subject to: } &amp; -A^T y &amp; \\leq &amp; -c \\\\ &amp; y &amp; \\geq &amp; 0, \\end{array} \\end{equation*}\\] Using Equation (9.3) for this dictionary we get the range of optimality for \\(-c_j\\) to be \\[\\begin{equation} (-\\mathcal{B}_d)^{-1} (-c) + \\delta (-\\mathcal{B}_d^{-1})_{\\_j} \\ge 0. \\end{equation}\\] where \\(-\\mathcal{B}_d\\) is formed by combining the dual-basic columns of \\(\\begin{bmatrix} -A^T &amp; I_n\\end{bmatrix}\\) and so \\(\\mathcal{B}_d\\) is formed by combining the dual-basic columns of \\(\\begin{bmatrix} A^T &amp; -I_n\\end{bmatrix}\\). This simplifies to \\[\\begin{equation} \\mathcal{B}_d^{-1} c - \\delta (\\mathcal{B}_d^{-1})_{\\_j} \\ge 0. \\end{equation}\\] Note that this is the range of optimality for \\(-c_j\\). To get the range of optimality for \\(c_j\\) we need to replace \\(\\delta\\) by \\(-\\delta\\) to get \\[\\begin{equation} \\mathcal{B}_d^{-1} c + \\delta (\\mathcal{B}_d^{-1})_{\\_j} \\ge 0. \\end{equation}\\] Finally, by Lemma 9.1, \\(\\mathcal{B}_d^{-1} c\\) is the vector of values of the dual basic variables. Hence we get, Proposition 9.2 The range of optimality for \\(c_j\\) is the interval \\([c_j + \\delta_-, c_j + \\delta_+]\\) such that \\(y_{\\mathcal{B}}^* + \\delta (\\mathcal{B}_d^{-1})_{\\_j} \\ge 0\\) for all \\(\\delta \\in [\\delta_-, \\delta_+]\\). By a happy accident all the negative signs have cancelled out and the equation for finding the range of optimality for the objective coefficients is exactly the same as the one for finding the range of optimality for the constraints. Theorem 9.2 The range of optimality for the constraints and the objective functions can be computed using the following formulae: Range of optimality for \\(b_i\\): \\[\\begin{equation} x_{\\mathcal{B}}^* + \\delta (\\mathcal{B}^{-1})_{\\_i} \\ge 0. \\end{equation}\\] where \\(\\mathcal{B}\\) is formed by combining the primal-basic columns of \\(\\begin{bmatrix} A &amp; I_m \\end{bmatrix}\\), and Range of optimality for \\(c_j\\): \\[\\begin{equation} y_{\\mathcal{B}}^* + \\delta (\\mathcal{B}_d^{-1})_{\\_j} \\ge 0. \\end{equation}\\] where \\(\\mathcal{B}_d\\) is formed by combining the dual-basic columns of \\(\\begin{bmatrix} A^T &amp; -I_n \\end{bmatrix}\\). Finally, to find \\(\\mathcal{B}_d\\) we note that by Complementary Slackness (Theorem 8.4), if the linear program is non-degenerate then the dual basic variables correspond to the primal non-basic variables (as the dual basic variables must be non-zero). This statement is true even for degenerate linear programs but in this case the proof is more subtle and requires the use of Strong duality (Theorem 8.2). Example 9.5 Consider the linear program (1.2) again. At the optimal solution, as \\(w_1\\) and \\(w_2\\) are non-basic, the corresponding dual variables (\\(y_1\\) and \\(y_2\\)) will be basic for the dual linear program (7.2). This gives us \\[\\begin{align*} \\mathcal{B}_d &amp;= \\begin{bmatrix} 3 &amp; 2 \\\\ 6 &amp; 1 \\end{bmatrix} \\\\ \\implies \\mathcal{B}_d^{-1} &amp;= \\begin{bmatrix} -1/9 &amp; 2/9 \\\\ 2/3 &amp; -1/3 \\end{bmatrix}. \\end{align*}\\] From Example 9.4 we know that \\(y_1 = 2/9\\) and \\(y_2 = 5/3\\) at the dual optimal solution. Using these, we can now find the range of optimality for the objective coefficients. To find the range of optimality for \\(c_1 = 4\\) we solve \\[\\begin{align*} \\begin{bmatrix} 2/9 \\\\ 5/3 \\end{bmatrix} + \\delta \\begin{bmatrix} -1/9 \\\\ 2/3 \\end{bmatrix} \\ge 0 \\end{align*}\\] which gives us the inequalities \\[\\begin{align*} \\begin{array}{lrlrrll} 2/9 + \\delta (-1/9) &amp; \\ge &amp; 0 &amp; \\implies &amp; \\delta &amp; \\le &amp; 2 \\\\ 5/3 + \\delta (2/3) &amp; \\ge &amp; 0 &amp; \\implies &amp; \\delta &amp; \\ge &amp; -5/2 \\end{array} \\end{align*}\\] So, \\(\\delta_- = -5/2\\) and \\(\\delta_+ = 2\\) and the range of optimality for \\(c_1\\) is \\([4 - 5/2, 4 + 2] = [1.5, 6]\\). To find the range of optimality for \\(c_2 = 3\\) we solve \\[\\begin{align*} \\begin{bmatrix} 2/9 \\\\ 5/3 \\end{bmatrix} + \\delta \\begin{bmatrix} 2/9 \\\\ -1/3 \\end{bmatrix} \\ge 0 \\end{align*}\\] which gives us the inequalities \\[\\begin{align*} \\begin{array}{lrlrrll} 2/9 + \\delta (2/9) &amp; \\ge &amp; 0 &amp; \\implies &amp; \\delta &amp; \\ge &amp; -1 \\\\ 5/3 + \\delta (-1/3) &amp; \\ge &amp; 0 &amp; \\implies &amp; \\delta &amp; \\le &amp; 5 \\end{array} \\end{align*}\\] So, \\(\\delta_- = -1\\) and \\(\\delta_+ = 5\\) and the range of optimality for \\(c_2\\) is \\([3 - 1, 3 + 5] = [2, 8]\\). The following figures show the (infinitely many) optimal solutions at the extreme ends of the range of optimality of \\(c_1\\). "],["convex-programming.html", "Chapter 10 Convex Programming", " Chapter 10 Convex Programming We will now move toward generalizing the concepts from linear programming to non-linear optimization questions. Well only touch several subjects briefly without going into too much depth. Well start with convex optimization, which is a direct generalization of linear programming. Definition 10.1 We say that a subset \\(\\mathcal{S}\\) of \\(\\mathbb{R}^n\\) is called convex if for any points \\({x}_1\\) and \\({x}_2\\) in \\(\\mathcal{S}\\), the line segment connecting them also lies in \\(\\mathcal{S}\\). More precisely, we say that \\(S\\) is convex if forall \\({x}_1\\) in \\(\\mathcal{S}\\), \\({x}_2\\) in \\(\\mathcal{S}\\), and \\(t \\in [0,1]\\), the point \\((1 - t) {x}_1 + t {x}_2\\) also lies in \\(\\mathcal{S}\\). Exercise 10.1 Show that the feasible region of a linear program is convex. Definition 10.2 We say that function \\(f : \\mathcal{S} \\to \\mathbb{R}\\) is convex if for all \\({x}_1\\) in \\(\\mathcal{S}\\), \\({x}_2\\) in \\(\\mathcal{S}\\), and \\(t \\in [0,1]\\), the inequality \\(f((1 - t) {x}_1 + t {x}_2) \\le (1 - t) f({x}_1) + t f({x}_2)\\) holds. Geometrically, this is saying that any line segment connecting two points on the graph of \\(f\\) lies above the graph. The following theorem provides a quick way to check if a function is convex and to come up with examples of convex functions. Theorem 10.1 Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) be a \\(C^2\\) function. \\(f\\) is convex if and only if the Hessian matrix of \\(f\\) is positive semi-definite. In particular, when \\(n = 1\\), \\(f\\) is convex if and only if \\(f&#39;&#39; \\ge 0\\). Definition 10.3 An optimization problem \\[\\begin{equation} \\begin{array}{ll} \\mbox{maximize: } &amp; f(x) \\\\ \\mbox{subject to: } &amp; x \\in \\mathcal{S} \\end{array} \\tag{10.1} \\end{equation}\\] is called convex if \\(\\mathcal{S}\\) is a closed and convex subset of \\(\\mathbb{R}^n\\) and \\(f : \\mathcal{S} \\to \\mathbb{R}\\) is a convex function. Exercise 10.2 Show that a linear program is a convex optimization problem. The following theorem is an example of how theorems about linear programs generalize to convex programs. Theorem 10.2 The convex optimization problem (10.1) either has an optimal solution on the boundary of \\(\\mathcal{S}\\) or is unbounded. Proof. Let \\({x}\\) be a point in the interior of \\(\\mathcal{S}\\). It suffices to show that either there is some point on the boundary of \\(\\mathcal{S}\\) with an objective value that is greater than or equal to the objective value of \\({x}\\) or the problem is unbounded. Let \\({x}_1\\) be any point on the boundary of \\(\\mathcal{S}\\). Draw a ray starting at \\({x}_1\\) in the direction of \\({x}\\). As \\(\\mathcal{S}\\) is closed and convex, two cases are possible: The ray intersects the boundary of \\(\\mathcal{S}\\) in exactly one more point, say \\({x}_2\\). The ray does not intersect the boundary of \\(\\mathcal{S}\\) and the entire ray is contained within \\(\\mathcal{S}\\). Case 1: We will show that either \\(f({x}) \\le f({x}_1)\\) or \\(f({x}) \\le f({x}_2)\\). We prove this by contradiction. Suppose \\(f({x}) &gt; f({x}_1)\\) and \\(f({x}) &gt; f({x}_2)\\). We know that \\(x = (1 - t) x_1 + t x_2\\) for some \\(t \\in (0, 1)\\). Multiplying the first inequality by \\((1-t)\\) and the second by \\(t\\) and adding them together, we get \\[\\begin{align*} &amp;&amp; (1-t) f({x}) + t f({x}) &amp;&gt; (1-t) f({x}_1) + t f({x}_2) \\\\ \\implies &amp;&amp; f({x}) &amp;&gt; (1-t) f({x}_1) + t f({x}_2) , \\end{align*}\\] which contradicts the convexity of \\(f\\). Case 2: If \\(f(x_1) \\ge f(x)\\), were done. Suppose this is not the case. Well show that the problem is unbounded. For \\(0 &lt; t \\le 1\\), let \\({y}_t\\) denote the point on the ray such that \\[\\begin{align*} {x} = (1-t) {x}_1 + t {y}_t. \\end{align*}\\] For example, \\({y}_1 = {x}\\) and \\({y}_{1/2}\\) is the point on the ray for which \\({x}\\) is midpoint of \\({x}_1\\) and \\({y}_{1/2}\\). As \\(t\\) decreases, \\({y}_t\\) will move farther and farther away from \\({x}\\). Note that all of \\({y}_t\\) lie inside \\(\\mathcal{S}\\). By convexity of \\(f\\), we know that \\[\\begin{align*} &amp;&amp; f({x}) &amp; \\le (1-t) f({x}_1) + t f({y}_ t) \\\\ \\implies &amp;&amp; f({x}) - (1-t) f({x}_1) &amp;\\le t f({y}_t) \\\\ \\implies &amp;&amp; t^{-1}f({x}) - (t^{-1}-1) f({x}_1) &amp;\\le f({y}_t) \\\\ \\implies &amp;&amp; t^{-1}(f({x}) - f({x}_1)) + f({x}_1) &amp;\\le f({y}_t) \\\\ \\end{align*}\\] As \\(f({x}) &gt; f({x}_1)\\), the left hand side tends to \\(\\infty\\) as \\(t\\) decreases and the optimization problem is unbounded. The following corollary follows by applying Extreme Value Theorem to the above result. Theorem 10.3 If \\(\\mathcal{S}\\) is bounded (in addition to being closed and convex), then the convex optimization problem (10.1) has an optimal solution on the boundary of \\(\\mathcal{S}\\). "],["separation-theorems.html", "Chapter 11 Separation Theorems 11.1 Farkas Lemma 11.2 Separating Hyperplane Theorem 11.3 Equivalence with Strong Duality", " Chapter 11 Separation Theorems Well next prove a couple of geometric results that are equivalent to strong duality theorem (Theorem 8.2). 11.1 Farkas Lemma Theorem 11.1 (Farkas' lemma) Let \\(A\\) be an \\(m \\times n\\) matrix and \\(b\\) be a vector in \\(\\mathbb{R}^m\\). Exactly one of the following systems has a solution: \\[\\begin{align} A x &amp; = b \\\\ x &amp; \\ge 0. \\tag{11.1} \\end{align}\\] \\[\\begin{align} y^T A &amp; \\ge 0 \\\\ y^T b &amp; &lt; 0. \\tag{11.2} \\end{align}\\] Proof. Well prove Farkas lemma using strong duality. Consider the following linear program: \\[\\begin{equation} \\begin{array}{llll} \\mbox{maximize: } &amp; 0 \\\\ \\mbox{subject to: } &amp; A x &amp; = &amp; b \\\\ &amp; x &amp; \\ge &amp; 0. \\end{array} \\tag{11.3} \\end{equation}\\] The optimal solution to the linear program (11.3) is a feasible solution to (11.1). The dual to this linear program is \\[\\begin{equation} \\begin{array}{llll} \\mbox{minimize: } &amp; y^T b \\\\ \\mbox{subject to: } &amp; y^T A &amp; \\ge &amp; 0. \\end{array} \\tag{11.4} \\end{equation}\\] Case 1: Suppose (11.1) has a solution. In this case, (11.3) has an optimal solution. By strong duality, (11.4) also has an optimal solution with optimal objective \\(0\\). So, the minimum value of \\(y^T b\\) is \\(0\\) and hence the system (11.2) does not have a solution. Case 2: Suppose (11.1) does not have a solution. In this case, (11.3) has no optimal solution. By strong duality, neither does (11.4). So, (11.4) is either infeasible or unbounded. As \\(y = 0\\) is a feasible solution to (11.4) it cannot be infeasible, and hence it must be unbounded. But this means that the value of \\(y^T b\\) can be made arbitrarily small, and in particular, can be made negative. Hence, the system (11.2) has a solution. We can interpret Farkas lemma geometrically using convex cones and separating hyperplanes. 11.2 Separating Hyperplane Theorem Definition 11.1 The convex cone of a finite set of vectors in \\(\\mathbb{R}^m\\) is the set of positive linear combinations of vectors in the set.2 \\[\\begin{align*} C_+(v_1, \\dots, v_n) &amp; := \\{c_1 v_1 + \\dots + c_n v_n \\mid c_i \\ge 0 \\}. \\end{align*}\\] Definition 11.2 A hyperplane in \\(\\mathbb{R}^m\\) is the set of solutions to a single linear equation. The complement of a hyperplane in \\(\\mathbb{R}^n\\) consists of two connected components. The closures of these components are called half-spaces. If the equation of the hyperplane is given by \\(b^T y = b_0\\), where \\(b\\) is a vector in \\(\\mathbb{R}^m\\) and \\(b_0 \\in \\mathbb{R}\\), then the corresponding two half-spaces are described by \\(b^T y \\le b_0\\) and \\(b^T y \\ge b_0\\). Definition 11.3 We say that a hyperplane \\(H\\) separates two subsets \\(S_1\\) and \\(S_2\\) of \\(\\mathbb{R}^m\\) if \\(S_1\\) and \\(S_2\\) do not intersect \\(H\\) and belong to the two different half-spaces of \\(H\\). Using convex cones and separating hyperplanes, we can reinterpret 11.1 as follows. Theorem 11.2 (Geometric version of Farkas' lemma) Let \\(v_1, \\dots, v_n, b\\) be vectors in \\(\\mathbb{R}^m\\). Exactly one of the following statements is true: Either \\(b\\) lies inside \\(C_+(v_1, \\dots, v_n)\\), or There is a hyperplane \\(H\\) that separates \\(b\\) from \\(C_+(v_1, \\dots, v_n)\\). A point \\(b\\) and a convex cone \\(C_+(v_1, \\dots, v_n)\\) are convex subsets of \\(\\mathbb{R}^m\\). The statement of Farkas lemma 11.2 can be generalized to arbitrary convex sets using metric topology. For now, well generalize it to convex polyhedra. Definition 11.4 Intersection of finitely many half-spaces is called a convex polyhedron. So, a convex polyhedron is the set of solutions to a system of linear inequalities \\(A x \\le b\\). But this set is precisely the feasible region of a linear program! The class of convex polyhedra is very large and all geometric linear convex objects, like points, lines, planes, half-spaces, hyperplanes, convex cones, etc. can be realized as convex polyhedra. The following is an extension of Farkas lemma to convex polyhedra (proof in the exercises below). Theorem 11.3 (Separating Hyperplane Theorem) Any two non-empty, disjoint, convex polyhedra in \\(\\mathbb{R}^m\\) can be separated by a hyperplane. Exercise 11.1 Prove the following variant of Farkas lemma: Let \\(A\\) be an \\(m \\times n\\) matrix and let \\(b\\) be a vector in \\(\\mathbb{R}^m\\). Exactly one of the following systems has a solution: \\(Ax \\le b\\), \\(y^T b &lt; 0\\), \\(y^T A = 0\\), and \\(y \\ge 0\\). Exercise 11.2 Let \\(P_1\\) be the convex polyhedron defined by \\(A_1 x \\le b_1\\) and let \\(P_2\\) be the convex polyhedron defined by \\(A_2 x \\le b_2\\) where \\(A_1\\), \\(A_2\\), \\(b_1\\), and \\(b_2\\) have sizes \\(m_1 \\times n\\), \\(m_2 \\times n\\), \\(m_1\\), and \\(m_2\\), respectively. Suppose the system \\[\\begin{equation} \\begin{bmatrix} A_1 \\\\ A_2 \\end{bmatrix} x \\le \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix} \\tag{11.5} \\end{equation}\\] does not have a solution. Apply the above variant of Farkas lemma to (11.5) to obtain vectors \\(y_1\\), \\(y_2\\) of size \\(m_1\\) and \\(m_2\\), respectively. Define \\(c^T := y_1^T A_1\\), \\(d_1 := y_1^T b_1\\), and \\(d_2 := -y_2^T b_2\\). Show that \\(d_1 &lt; d_2\\). Show that \\(A_1 x \\le b_1\\) implies \\(c^T x \\le d_1\\). Show that \\(A_2 x \\le b_2\\) implies \\(c^T x \\ge d_2\\). Conclude that there exists a hyperplane separating \\(P_1\\) and \\(P_2\\). 11.3 Equivalence with Strong Duality It is possible to prove Strong Duality using Farkas lemma, which itself can be proven using metric topology. So, Strong Duality, Farkas lemma, and Separating Hyperplane Theorem should all be thought of as equivalent to each other. We provide below a proof of Strong Duality (Theorem 8.2) using Farkas lemma. Proof. Consider the system of equations \\[\\begin{equation} \\begin{split} Ax &amp; \\le b \\\\ -A^T y &amp; \\le -c \\\\ x, y &amp; \\ge 0 \\\\ -c^T x + b^T y &amp; \\le 0. \\end{split} \\tag{11.6} \\end{equation}\\] Suppose this system has a feasible solution. The first three equations are equivalent to saying that \\(x\\) is primal-feasible and \\(y\\) is dual-feasible. If such a solution exists, by Weak Duality (Theorem 8.1) we know that \\(c^T x \\le b^T y\\). So, the only way the fourth inequality is satisfied is if \\(c^T x = b^T y\\) i.e. the primal-objective value at \\(x\\) equals the dual-objective value at \\(y\\). But by Weak Duality, these must then be the optimal solutions thereby proving Strong Duality. So, it suffices to show that (11.6) has a feasible solution if the primal has an optimal solution. We prove this by contradiction. Suppose the primal has an optimal solution but (11.6) does not have a feasible solution. We rewrite the system as \\[\\begin{align*} \\begin{bmatrix} A &amp; 0 \\\\ 0 &amp; -A^T \\\\ -c^T &amp; b^T \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} &amp; \\le \\begin{bmatrix} b \\\\ -c \\\\ 0 \\end{bmatrix} \\\\ x, y &amp; \\ge 0. \\end{align*}\\] By (a variant of) Farkas lemma, the following dual system must have a solution. \\[\\begin{align*} \\begin{bmatrix} z^T &amp; w^T &amp; t \\end{bmatrix} \\begin{bmatrix} b \\\\ -c \\\\ 0 \\end{bmatrix} &amp; &lt; 0 \\\\ \\begin{bmatrix} z^T &amp; w^T &amp; t \\end{bmatrix} \\begin{bmatrix} A &amp; 0 \\\\ 0 &amp; -A^T \\\\ -c^T &amp; b^T \\end{bmatrix} &amp; \\ge 0\\\\ z, w, t &amp; \\ge 0. \\end{align*}\\] which can be rewritten as \\[\\begin{align*} z^T b &amp; &lt; w^T c \\\\ z^T A &amp; \\ge t c^T \\\\ t b^T &amp; \\ge w^T A^T \\\\ z, w, t &amp; \\ge 0. \\end{align*}\\] By combining the second and third equations, we get \\[\\begin{align*} t z^T b \\ge z^T A w \\ge t c^T w. \\end{align*}\\] If \\(t &gt; 0\\), this contradicts the first equation and were done. So suppose \\(t = 0\\). Plugging in \\(t = 0\\), we get \\[\\begin{align*} z^T b &amp; &lt; w^T c \\\\ z^T A &amp; \\ge 0 \\\\ 0 &amp; \\ge w^T A^T \\\\ z, w &amp; \\ge 0. \\end{align*}\\] which can be rewritten as the matrix equation \\[\\begin{align*} \\begin{bmatrix} z^T &amp; w^T \\end{bmatrix} \\begin{bmatrix} b \\\\ -c \\end{bmatrix} &amp; &lt; 0 \\\\ \\begin{bmatrix} z^T &amp; w^T \\end{bmatrix} \\begin{bmatrix} A &amp; 0 \\\\ 0 &amp; -A^T \\end{bmatrix} &amp; \\ge 0\\\\ z, w &amp; \\ge 0. \\end{align*}\\] Because this system has a solution, by applying (variant of) Farkas lemma again, the following system cannot have a solution. \\[\\begin{align*} \\begin{bmatrix} A &amp; 0 \\\\ 0 &amp; -A^T \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} &amp; \\le \\begin{bmatrix} b \\\\ -c \\end{bmatrix} \\\\ x, y &amp; \\ge 0. \\end{align*}\\] We had assumed that the primal is feasible. The only way the above system is infeasible is if there is no solution no the system \\[\\begin{align*} A^T y &amp;\\ge c \\\\ y &amp;\\ge 0. \\end{align*}\\] Applying (variant of) Farkas lemma yet again we see that the dual system \\[\\begin{align*} z^T c &amp; &lt; 0\\\\ z^T A^T &amp;\\ge 0 \\\\ z &amp;\\le 0 \\end{align*}\\] must have a solution. But now, if \\(x\\) is any primal-feasible solution then so is \\(x - \\alpha z\\) for any positive constant \\(\\alpha\\) and the objective value of the primal at \\(x - \\alpha z\\) can be made arbitrarily large by increasing \\(\\alpha\\) thereby contradicting the fact that primal has an optimal solution (and hence is bounded). One can also define convex cones for infinite sets of vectors. In this case, we need to take the closure of the set of positive linear combinations. "],["interior-point-methods.html", "Chapter 12 Interior Point Methods 12.1 Gradient Descent 12.2 Interior Point Method", " Chapter 12 Interior Point Methods Throughout this chapter we will let \\(f\\) denote a twice differentiable function from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}\\). 12.1 Gradient Descent We will start by trying to solve the following unconstrained optimization problem: \\[\\begin{align*} \\mbox{minimize: } &amp; f(x) \\end{align*}\\] where \\(x\\) is any vector in \\(\\mathbb{R}^n\\). Gradient descent is a simple algorithm for solving this problem using basic differential calculus. It relies on the fact that the negative of the gradient points in the direction in which the function decreases the fastest. So the general principle is to move in the direction of the negative gradient until the function is no longer decreasing. More precisely, we create a sequence of guesses using the following GD recurrence relation: \\[x_{k+1} = x_k - t_k \\nabla f(x_k)\\] where \\(t_k\\) is the step size for the \\(k^{th}\\) iteration and can be chosen to be a small constant or some function of \\(k\\), \\(x_k\\), \\(f(x_k)\\), or \\(\\nabla f(x_k)\\). There are several issues with this technique: If the function has multiple local minima, then the sequence might converge to a non-absolute minima depending on the starting guess and the choice of step sizes. If the step sizes are chosen to be too large, then the sequence can completely miss the minima and may not converge. If the step sizes are chosen to be too small, then the sequence may take a long time to converge. Unfortunately, there is no easy way to resolve these issues. In practice, either we need some additional information about the function and its gradient or we proceed by trial and error to find step sizes that work. Even then there is no guarantee that the algorithm will converge to an absolute minima and not a local minima, if at all. In spite of these issues, because of its simplicity, ease of implementation, and good convergence properties in practice, Gradient Descent is a very popular algorithm for solving unconstrained optimization problems. 12.2 Interior Point Method Gradient descent can be modified to solve constrained optimization problems by introducing barrier functions. Consider the following problem: \\[\\begin{equation} \\begin{array}{llr} \\mbox{minimize: } &amp; f(x) \\\\ \\mbox{subject to: } &amp; g_i(x) \\ge 0, &amp; \\mbox{ for } 1 \\le i \\le m. \\\\ \\end{array} \\tag{12.1} \\end{equation}\\] We can apply GD to this problem and find a critical point for \\(f(x)\\). However, this might not answer the optimization question for two reasons: The critical point might not be in the feasible region. The optimal solution might not be obtained at a critical of \\(f(x)\\) and could lie on the boundary \\(g(x) = 0\\) of the feasible region. Gradient descent algorithm only sees the objective function and does not know about the constraints. So, we modify the objective function to include the constraints using barrier functions. Definition 12.1 A barrier function is a differentiable function \\(b\\) from \\((0, \\infty)\\) to \\(\\mathbb{R}\\) that has the property \\(\\lim \\limits_{x \\to 0^+} f(x) = \\infty\\). Well use the barrier function \\(-\\ln x\\). Using a small positive parameter \\(\\mu\\), we create a new objective function: \\[\\begin{align*} f_\\mu(x) := &amp; f(x) - \\mu \\sum \\limits_{i = 1} ^ m \\ln (g_i(x)). \\end{align*}\\] Consider the unconstrained optimization problem \\[\\begin{align*} \\mbox{minimize: } &amp; f_\\mu (x). \\end{align*}\\] Because the domain of \\(\\ln x\\) is \\((0, \\infty)\\), any critical point of \\(f_\\mu(x)\\) must lie in the feasible region of (12.1). Let \\(x_\\mu^*\\) be a solution of the above problem. For sufficiently good functions \\(f\\) and \\(g_i\\), \\(x_\\mu^*\\) exists and is continuous in \\(\\mu\\) and \\(\\lim \\limits_{\\mu \\to 0^+} x_\\mu^* = x^*\\) where \\(x^*\\) is an optimal solution to (12.1). This assumption is valid, for example, for linear programs. With this assumption, we now have a method for solving (12.1): Start with a small \\(\\mu_0 &gt; 0\\) and optimize \\(f_{\\mu_0}(x)\\) using GD. Call the solution \\(x_{\\mu_0}^*\\). Repeat the following until the sequence \\(x_{\\mu_k}^*\\) stabilizes sufficiently: Set \\(\\mu_{k+1} = \\mu_k - \\delta_k\\) for some small \\(\\delta_k\\). Using \\(x^*_{\\mu_k}\\) as a starting point for GD, optimize \\(f_{\\mu_{k+1}}(x)\\). Call the solution \\(x_{\\mu_{k+1}}^*\\). Increase \\(k\\) to \\(k + 1\\). This method is a very simplified interior point method. The sequence of points \\(x_{\\mu}\\) is called the central path. Example 12.1 We can compute the central path explicitly for some simple examples. Consider the optimization problem: \\[\\begin{equation} \\begin{split} \\mbox{minimize: } &amp; (x + 1)^2 \\\\ \\mbox{subject to: } &amp; x \\ge 0. \\end{split} \\tag{12.2} \\end{equation}\\] We can calculate the central path as follows: \\[\\begin{align*} f_\\mu(x) := (x + 1)^2 - \\mu \\ln (x). \\end{align*}\\] The critical points for this function can be obtained by setting the derivative to 0. \\[\\begin{align*} f&#39;_\\mu(x) &amp;= 0 \\\\ \\implies 2(x + 1) - \\dfrac{\\mu}{x} &amp;= 0 \\\\ \\implies x^2 + x - \\mu/2 &amp;= 0 \\\\ \\implies x &amp;= \\dfrac{-1 \\pm \\sqrt{1 + 2 \\mu}}{2}. \\end{align*}\\] Only one of the two satisfy \\(x \\ge 0\\), so we get \\[\\begin{align*} x_\\mu^* = \\dfrac{-1 + \\sqrt{1 + 2 \\mu}}{2}. \\end{align*}\\] This is the central path. Taking the limit as \\(\\mu \\to 0\\) we get, \\[\\begin{align*} \\lim \\limits_{\\mu \\to 0^+} x_\\mu^* &amp; = \\lim \\limits_{\\mu \\to 0^+} \\dfrac{-1 + \\sqrt{1 + 2 \\mu}}{2} \\\\ &amp;= 0, \\end{align*}\\] which is indeed the optimal solution for the optimization problem (12.2). "],["kkt-conditions.html", "Chapter 13 KKT conditions", " Chapter 13 KKT conditions We can use the central path to derive some conditions for optimality. Consider the optimization problem (12.1) again. As before, well assume that there is a central path \\(x_\\mu^*\\) converging to an optimal solution \\(x^*\\). Because \\(x_\\mu^*\\) is a critical point of \\(f_\\mu(x)\\), we get \\[\\begin{align*} \\nabla_x f_\\mu(x_\\mu^*) &amp;= 0 \\\\ \\implies \\nabla f(x_\\mu^*) - \\sum \\limits_{i = 1}^m \\dfrac{\\mu}{g_i(x_\\mu^*)} \\nabla g_i (x_\\mu^*)&amp;= 0 \\end{align*}\\] Applying \\(\\lim \\limits_{\\mu \\to 0^+}\\) to both sides, we get \\[\\begin{align*} \\lim \\limits_{\\mu \\to 0^+} \\nabla f(x_\\mu^*) - \\sum \\limits_{i = 1}^m \\lim \\limits_{\\mu \\to 0^+}\\dfrac{\\mu}{g_i(x_\\mu^*)} \\nabla g_i (x_\\mu^*)&amp;= 0 \\end{align*}\\] which simplifies to \\[\\begin{align*} \\nabla f(x^*) = \\sum \\limits_{i = 1}^m \\lambda_i \\nabla g_i (x^*) \\end{align*}\\] where \\(\\lambda_i = \\lim \\limits_{\\mu \\to 0^+}\\frac{\\mu}{g_i(x_\\mu^*)}\\) for \\(1 \\le i \\le m\\). (There are several assumptions here about the existence and convergence of limits that were brushing under the rug.) We can further analyze the constants \\(\\lambda_i\\). \\[\\begin{align*} &amp;&amp; \\dfrac{\\mu}{g_i(x_\\mu^*)} \\cdot g_i(x_\\mu^*) &amp;= \\mu \\\\ \\implies &amp;&amp; \\lim \\limits_{\\mu \\to 0^+}\\dfrac{\\mu}{g_i(x_\\mu^*)} \\cdot \\lim \\limits_{\\mu \\to 0^+}g_i(x_\\mu^*) &amp;= \\lim \\limits_{\\mu \\to 0^+}\\mu \\\\ \\implies &amp;&amp;\\lambda_i g_i(x^*) &amp;= 0. \\end{align*}\\] Finally, because \\(g_i(x_\\mu^*) \\ge 0\\) and \\(\\mu &gt; 0\\), we must have \\[\\begin{align*} \\lambda_i \\ge 0. \\end{align*}\\] To summarize, if \\(x^*\\) is an optimal solution to (12.1) and there exists a central path \\(x_\\mu^*\\) converging to \\(x^*\\), then the following conditions are satisfied: \\[\\begin{align*} \\nabla f(x^*) &amp;= \\sum \\limits_{i = 1}^m \\lambda_i \\nabla g_i (x^*) \\\\ \\lambda_i g_i(x^*) &amp;= 0 \\\\ \\lambda_i &amp;\\ge 0. \\end{align*}\\] These are called the KKT conditions. We can combine the above derivation with Lagrange multipliers to get the following generalization: Theorem 13.1 (KKT conditions) Consider the optimization problem: \\[\\begin{equation*} \\begin{array}{llr} \\mbox{minimize: } &amp; f(x) \\\\ \\mbox{subject to: } &amp; g_i(x) \\ge 0, &amp; \\mbox{ for } 1 \\le i \\le m \\\\ &amp; h_i(x) = 0, &amp; \\mbox{ for } 1 \\le i \\le k. \\end{array} \\end{equation*}\\] Assume that this problem has an optimal solution \\(x^*\\) and there is a central path \\(x_\\mu^*\\) converging to \\(x^*\\). Then under certain regularity conditions, the following conditions are satisfied: \\[\\begin{align*} \\nabla f(x^*) &amp;= \\sum \\limits_{i = 1}^m \\lambda_i \\nabla g_i (x^*) + \\sum \\limits_{i = 1}^k \\lambda&#39;_i \\nabla h_i (x^*) \\\\ \\lambda_i g_i(x^*) &amp;= 0, \\mbox{ for } 1 \\le i \\le m \\\\ \\lambda_i &amp;\\ge 0, \\mbox{ for } 1 \\le i \\le m. \\end{align*}\\] The conditions for necessity, sufficiency and the regularity conditions are quite non-trivial and are beyond the scope of this course. Example 13.1 Consider Example 12.1 again. The KKT conditions for this example are \\[\\begin{align*} 2(x + 1) &amp;= \\lambda \\\\ \\lambda x &amp;= 0 \\\\ \\lambda &amp;\\ge 0. \\end{align*}\\] These need to be satisfied in addition to the original constraint \\(x \\ge 0\\). The only solution to this system is \\(x = 0\\), which is indeed the optimal solution. Example 13.2 Consider the linear program \\[\\begin{align*} \\mbox{minimize: } &amp; c^T x \\\\ \\mbox{subject to: } &amp; Ax \\ge b \\end{align*}\\] The KKT conditions for this problem are \\[\\begin{align*} c &amp;= A^T \\lambda \\\\ x_i \\lambda _i &amp;= 0, \\mbox{ for } 1 \\le i \\le n,\\\\ \\lambda &amp;\\ge 0. \\end{align*}\\] This is precisely the dual of the linear programming problem and complementary slackness conditions. Thus the KKT conditions recover duality in the linear programming sense. Exercise 13.1 Find the KKT conditions for the standard linear program \\[\\begin{align*} \\mbox{maximize: } &amp; c^T x \\\\ \\mbox{subject to: } &amp; Ax \\le b \\\\ &amp; x \\ge 0. \\end{align*}\\] Explain how these conditions recover the dual constraints and complementary slackness. "],["l1-regression.html", "Chapter 14 L1-Regression", " Chapter 14 L1-Regression Linear regression is a method for modelling the relationship between outputs and inputs using linear functions. Suppose that the data set consists of the points \\((x_i, y_i)\\) with \\(i = 1, 2, ..., N\\) where each \\(x_i\\) is itself a vector in \\(\\mathbb{R}^m\\). We want to find a function \\(f : \\mathbb{R}^m \\to \\mathbb{R}\\) such that \\(f(x_{i}) \\approx y_{i}\\). In linear regression, we suppose that the function \\(f\\) is of the form \\[f(x) = \\beta^T x + \\beta_0,\\] where \\(\\beta\\) is a vector in \\(\\mathbb{R}^m\\) and \\(\\beta_0\\) is a constant. Our goal is to estimate \\(\\beta\\) and \\(\\beta_0\\). There is of course, no reason that there is a linear relation between the inputs \\(x_i\\) and the outputs \\(y_i\\). So our is simply to find the best estimate. We can define the error in estimation to be the vector \\(\\varepsilon\\) whose components are \\[\\begin{align*} \\varepsilon_i = \\lvert y_i - f(x_i) \\rvert. \\end{align*}\\] We wish to minimize the vector in \\(\\varepsilon\\). There are multiple measures one can define on the space of these errors and minimizing each provides us a best estimate in the sense of that particular measure. The most commonly used measure is the Euclidean distance or the \\(L^2\\)-norm. We define \\[\\begin{align*} \\| \\varepsilon_i \\|_2 = \\sum \\limits_{i = 1}^N (y_i - f(x_i))^2 . \\end{align*}\\] In \\(L^2\\)-regression we try to find the function \\(f\\) that minimizes this quantity. There are several reasons why this measure is most commonly used, one of which being that there is a closed form for the solution using the normal equation. However, the solution obtained from \\(L^2\\)-regression is very susceptible to outliers. By comparison, the \\(L^1\\)-measure, defined below, is more robust to outliers. \\[\\begin{align*} \\| \\varepsilon_i \\|_1 = \\sum \\limits_{i = 1}^N \\lvert y_i - f(x_i) \\rvert. \\end{align*}\\] The biggest downside of \\(L^1\\) is that it does not have a closed form solution. However, it is possible to reduce the problem of finding the best \\(L^1\\)-estimate to linear programming. Our goal is to solve the following unconstrained minimization problem \\[\\begin{align*} \\mbox{minimize:} \\sum \\limits_{i = 1}^N \\lvert y_i - (\\beta^T x_i + \\beta_0) \\rvert, \\end{align*}\\] where our variables are \\(\\beta\\) and \\(\\beta_0\\). As stated, this problem is not a linear program. However, it is equivalent to the following linear program: \\[\\begin{equation*} \\begin{array}{lllr} \\mbox{minimize:} &amp; \\sum \\limits_{i = 1}^N \\varepsilon_i \\\\ \\mbox{subject to:} &amp; y_i - (\\beta^T x_i + \\beta_0) &amp; &lt; &amp; \\varepsilon_i \\\\ &amp; y_i - (\\beta^T x_i + \\beta_0) &amp; &gt; &amp; -\\varepsilon_i. \\end{array} \\end{equation*}\\] Here the decision variables are \\(\\beta\\) (which is a vector in \\(\\mathbb{R}^m\\)), \\(\\beta_0\\), and \\(\\varepsilon_i\\) for \\(1 \\le i \\le N\\). "],["network-flow.html", "Chapter 15 Network Flow 15.1 Min-cut", " Chapter 15 Network Flow One very important application of linear programming is to finding max-flows in graphs. The setup is as follows: we have a simple directed graph \\(G = (V, E)\\) where \\(V\\) is the set of vertices and \\(E \\subseteq V \\times V\\) is the set of directed edges. Each edge has a non-negative capacity given by a function \\(c: E \\to \\mathbb{R}_{\\ge 0}\\). For simplicity, we can assume that capacity is a function \\(c : V \\times V \\to \\mathbb{R}_{\\ge 0}\\) such that \\(c(v, v&#39;) = 0\\) if there is no edge going from \\(v\\) to \\(v&#39;\\). There are two special vertices called source and sink denotes \\(s\\) and \\(t\\), respectively. The source \\(s\\) has the property that every edge connected to the source points away from it i.e. there no incoming edges and the sink \\(t\\) has the property that every edge connected to the sink points toward it i.e. there no outgoing edges. Such a graph is called a network. A flow is a function \\(f : E \\to \\mathbb{R}\\) with the property that \\(f \\le c\\). As before, we can assume that \\(f\\) is a function \\(V \\times V \\to \\mathbb{R}\\). A flow is said to be balanced if at each vertex the incoming flow equals the outgoing flow. The max-flow question is to find a balanced flow that maximizes the net outflow from the source. More precisely, \\[\\begin{equation*} \\begin{array}{lrllllllllll} \\mbox{maximize:} &amp; \\sum \\limits_{v \\in V} f(s, v) \\\\ \\mbox{subject to:} &amp; f(v, w) &amp; \\le &amp; c(v, w) &amp; \\mbox{ for all } v, w \\in V \\\\ &amp; \\sum \\limits_{w \\in V} f(v, w) &amp; = &amp; \\sum \\limits_{w \\in V} f(w, v) &amp; \\mbox{ for all } v \\in V. \\end{array} \\end{equation*}\\] This is very clearly a linear program that can be solved using the simplex method. Note however, that several faster algorithms exist for solving this problem. 15.1 Min-cut The dual of the max-flow problem has an interesting interpretation. Suppose \\(y(v, w)\\) are the dual decision variables corresponding to the first kind of constraints and \\(z(v)\\) are the dual decision variables corresponding to the second kind of constraints. \\[\\begin{equation*} \\begin{array}{lrllllllllll} \\mbox{minimize:} &amp; \\sum \\limits_{(v, w) \\in V \\times W} c(v, w) y(v, w) \\\\ \\mbox{subject to:} &amp; y(v, w) &amp; \\ge &amp; z(v) - z(w) &amp; \\mbox{ for all } v, w \\in V \\\\ &amp; z(s) &amp;= &amp;1 \\\\ &amp;z(t) &amp;= &amp;0 \\\\ &amp; y(v,w) &amp; \\ge &amp; 0 &amp; \\mbox{ for all } v, w \\in V. \\end{array} \\end{equation*}\\] Consider the graph \\(G\\) again. A directed path from the source \\(s\\) to a sink \\(t\\) is a sequence of edges of the form \\((s, v_1)\\), \\((v_1, v_2)\\), \\(\\dots\\), \\((v_{k-1}, v_k)\\), \\((v_k, t)\\) in \\(E\\). A subset of edges \\(S \\subseteq E\\) is called an s-t cut if after deleting the edges in \\(S\\) there is no directed path from \\(s\\) to \\(t\\). Every s-t cut provides a feasible solution to the dual problem as follows: We set \\(y(v, w) = 1\\) if \\((v, w)\\) is in the cut, 0 otherwise. We set \\(z(v)\\) equal 1 if there is a path from \\(s\\) to \\(v\\) after making the cut \\(S\\), 0 otherwise. One can check that the constraints of the dual linear program are indeed satisfied by such a solution. In fact, a much stronger statement is true. The optimal solution to the dual problem is of this form i.e. it comes from an s-t cut. The proof of this statement is beyond the scope of this class. The size of an s-t cut is the sum of capacities of the edges in the cut. And thus the dual problem is equivalent to finding a min-cut in the graph \\(G\\). "],["integer-programming.html", "Chapter 16 Integer Programming 16.1 Branch and Bound", " Chapter 16 Integer Programming An integer program is a linear program in which some or all of the variables are required to be integers. The most common constraint is usually that some variable is required to be either 0 or 1. Even though integer programs are slight generalizations of linear programs, they are much harder to solve. It can be shown that solving a general integer program is an NP-hard problem i.e. we do not expect to there be any efficient algorithm to solve them. The best we can hope for is to analyze some special kinds of integer programs or to find approximation algorithm for the general case. One naive approach to solve an integer program is to simply forget the integrality condition and solve the underlying linear program. This technique is called LP-relaxation. For nice integer programs, we can expect this method to provide a good approximate solution. However, this method can also provide really incorrect answers. Consider the following integer program \\[\\begin{equation*} \\begin{array}{lllll} \\mbox{maximize:} &amp; x \\\\ \\mbox{subject to:} &amp; y &amp; \\le &amp; \\dfrac{x}{n-0.5} \\\\ &amp; x, y &amp; \\ge &amp; 0 \\\\ &amp; x, y &amp; &amp; \\mbox{ are integers,} \\end{array} \\end{equation*}\\] where \\(n\\) is a fixed integer. One can check that the feasible region for the LP-relaxation is a triangle with vertices \\((0,0)\\), \\((1,0)\\) and \\((n - 0.5, 1)\\) and the only integer points in this triangle are \\((0,0)\\) and \\((1,0)\\). So, the optimal solution to the LP-relaxation is \\((n - 0.5, 1)\\) but the optimal solution to the original integer program is \\((1,0)\\). Hence, as \\(n\\) increases the LP solution diverges away from the IP solution. 16.1 Branch and Bound One technique for fixing the issues with LP-relaxation is called brand-and-bound. In branch-and-bound we solve the LP-relaxation if we end with a non-integer solution, say with \\(x = k\\) then we make two cases \\(x \\le \\lfloor k \\rfloor\\) and \\(x \\ge \\lceil k \\rceil\\). The goal is to shrink the feasible region so that all the vertices have integer coordinates. We repeat the process for each case, essentially performing a DFS until we find integer solutions. This is best demonstrated with an example. Consider the integer program: \\[\\begin{equation} \\begin{array}{llllll} \\mbox{maximize:} &amp; 4x &amp; + &amp; 5y \\\\ \\mbox{subject to:} &amp; x &amp; + &amp; y &amp; \\le &amp; 10 \\\\ &amp; 3x &amp; - &amp; 4y &amp; \\le &amp; 6 \\\\ &amp; x &amp;, &amp; y &amp; \\ge &amp; 0 \\\\ &amp; x, y &amp; &amp; &amp; &amp; \\mbox{ are integers.} \\end{array} \\end{equation}\\] Solving the LP-relaxation gives us the solution \\((x,y) = (4, 1.5)\\). As the \\(y\\)-coordinates is not an integer, we make two cases \\(y \\le 1\\) and \\(y \\ge 2\\) and analyze each separately by adding it as an extra constraint. \\[\\begin{equation*} \\begin{array}{llllll} \\mbox{maximize:} &amp; 4x &amp; + &amp; 5y \\\\ \\mbox{subject to:} &amp; x &amp; + &amp; y &amp; \\le &amp; 10 \\\\ &amp; 3x &amp; - &amp; 4y &amp; \\le &amp; 6 \\\\ &amp; x &amp;, &amp; y &amp; \\ge &amp; 0 \\\\ &amp; x, y &amp; &amp; &amp; &amp; \\mbox{ are integers.} \\\\ &amp; &amp; &amp; y &amp; \\le &amp; 1 \\end{array} \\end{equation*}\\] In this case, the optimal solution to the LP-relaxation is \\((x,y) = (3.342, 1)\\). This time \\(x\\) is not an integer, so we make further two cases \\(x \\le 3\\) and \\(x \\ge 4\\). In the first case, we get \\[\\begin{equation*} \\begin{array}{llllll} \\mbox{maximize:} &amp; 4x &amp; + &amp; 5y \\\\ \\mbox{subject to:} &amp; x &amp; + &amp; y &amp; \\le &amp; 10 \\\\ &amp; 3x &amp; - &amp; 4y &amp; \\le &amp; 6 \\\\ &amp; x &amp;, &amp; y &amp; \\ge &amp; 0 \\\\ &amp; x, y &amp; &amp; &amp; &amp; \\mbox{ are integers.} \\\\ &amp; &amp; &amp; y &amp; \\le &amp; 1 \\\\ &amp;x &amp; &amp; &amp; \\le &amp; 3, \\end{array} \\end{equation*}\\] which has the optimal solution \\((x,y) = (3, 1)\\). In the second case, we get \\[\\begin{equation*} \\begin{array}{llllll} \\mbox{maximize:} &amp; 4x &amp; + &amp; 5y \\\\ \\mbox{subject to:} &amp; x &amp; + &amp; y &amp; \\le &amp; 10 \\\\ &amp; 3x &amp; - &amp; 4y &amp; \\le &amp; 6 \\\\ &amp; x &amp;, &amp; y &amp; \\ge &amp; 0 \\\\ &amp; x, y &amp; &amp; &amp; &amp; \\mbox{ are integers.} \\\\ &amp; &amp; &amp; y &amp; \\le &amp; 1 \\\\ &amp;x &amp; &amp; &amp; \\ge &amp; 4, \\end{array} \\end{equation*}\\] the LP-relaxation and hence the integer program is infeasible. Going back we are left to analyze the case \\(y \\ge 2\\). \\[\\begin{equation*} \\begin{array}{llllll} \\mbox{maximize:} &amp; 4x &amp; + &amp; 5y \\\\ \\mbox{subject to:} &amp; x &amp; + &amp; y &amp; \\le &amp; 10 \\\\ &amp; 3x &amp; - &amp; 4y &amp; \\le &amp; 6 \\\\ &amp; x &amp;, &amp; y &amp; \\ge &amp; 0 \\\\ &amp; x, y &amp; &amp; &amp; &amp; \\mbox{ are integers.} \\\\ &amp; &amp; &amp; y &amp; \\ge &amp; 2. \\end{array} \\end{equation*}\\] This has an optimal solution \\((x, y) = (2, 2)\\). Thus we have two possible candidates for optimal solutions to the integer program: \\((3, 1)\\) and \\((2, 2)\\). We check the objective value \\(4x + 3y\\) at each of these to conclude that \\((2, 2)\\) is indeed the optimal solution with objective value 18. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
