<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Interior Point Method | Introduction to Optimization</title>
  <meta name="description" content="Chapter 12 Interior Point Method | Introduction to Optimization." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Interior Point Method | Introduction to Optimization" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 12 Interior Point Method | Introduction to Optimization." />
  <meta name="github-repo" content="apurvanakade/Introduction-to-Optimization" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Interior Point Method | Introduction to Optimization" />
  
  <meta name="twitter:description" content="Chapter 12 Interior Point Method | Introduction to Optimization." />
  

<meta name="author" content="Apurva Nakade" />


<meta name="date" content="2022-06-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="separation-theorems.html"/>
<link rel="next" href="kkt-conditions.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="part"><span><b>I Simplex Method</b></span></li>
<li class="chapter" data-level="2" data-path="standard-linear-program.html"><a href="standard-linear-program.html"><i class="fa fa-check"></i><b>2</b> Standard Linear Program</a></li>
<li class="chapter" data-level="3" data-path="the-simplex-method.html"><a href="the-simplex-method.html"><i class="fa fa-check"></i><b>3</b> The Simplex Method</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-simplex-method.html"><a href="the-simplex-method.html#the-simplex-step"><i class="fa fa-check"></i><b>3.1</b> The Simplex Step</a></li>
<li class="chapter" data-level="3.2" data-path="the-simplex-method.html"><a href="the-simplex-method.html#tableau-notation"><i class="fa fa-check"></i><b>3.2</b> Tableau Notation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="initialization.html"><a href="initialization.html"><i class="fa fa-check"></i><b>4</b> Initialization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="initialization.html"><a href="initialization.html#auxiliary-linear-program"><i class="fa fa-check"></i><b>4.1</b> Auxiliary Linear Program</a></li>
<li class="chapter" data-level="4.2" data-path="initialization.html"><a href="initialization.html#combined-tableau"><i class="fa fa-check"></i><b>4.2</b> Combined tableau</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cycling.html"><a href="cycling.html"><i class="fa fa-check"></i><b>5</b> Cycling</a>
<ul>
<li class="chapter" data-level="5.1" data-path="cycling.html"><a href="cycling.html#degeneracy"><i class="fa fa-check"></i><b>5.1</b> Degeneracy</a></li>
<li class="chapter" data-level="5.2" data-path="cycling.html"><a href="cycling.html#blands-rule"><i class="fa fa-check"></i><b>5.2</b> Bland’s Rule</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="standardization.html"><a href="standardization.html"><i class="fa fa-check"></i><b>6</b> Standardization</a>
<ul>
<li class="chapter" data-level="6.1" data-path="standardization.html"><a href="standardization.html#equivalence-of-linear-programs"><i class="fa fa-check"></i><b>6.1</b> Equivalence of Linear Programs</a></li>
</ul></li>
<li class="part"><span><b>II Duality Theory</b></span></li>
<li class="chapter" data-level="7" data-path="dual-linear-program.html"><a href="dual-linear-program.html"><i class="fa fa-check"></i><b>7</b> Dual Linear Program</a>
<ul>
<li class="chapter" data-level="7.1" data-path="dual-linear-program.html"><a href="dual-linear-program.html#general-linear-program"><i class="fa fa-check"></i><b>7.1</b> General Linear Program</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="weak-and-strong-duality.html"><a href="weak-and-strong-duality.html"><i class="fa fa-check"></i><b>8</b> Weak and Strong Duality</a>
<ul>
<li class="chapter" data-level="8.1" data-path="weak-and-strong-duality.html"><a href="weak-and-strong-duality.html#weak-duality"><i class="fa fa-check"></i><b>8.1</b> Weak Duality</a></li>
<li class="chapter" data-level="8.2" data-path="weak-and-strong-duality.html"><a href="weak-and-strong-duality.html#strong-duality"><i class="fa fa-check"></i><b>8.2</b> Strong Duality</a></li>
<li class="chapter" data-level="8.3" data-path="weak-and-strong-duality.html"><a href="weak-and-strong-duality.html#complimentary-slackness"><i class="fa fa-check"></i><b>8.3</b> Complimentary slackness</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="sensitivity-analysis.html"><a href="sensitivity-analysis.html"><i class="fa fa-check"></i><b>9</b> Sensitivity Analysis</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sensitivity-analysis.html"><a href="sensitivity-analysis.html#dictionaries-revisited"><i class="fa fa-check"></i><b>9.1</b> Dictionaries Revisited</a></li>
<li class="chapter" data-level="9.2" data-path="sensitivity-analysis.html"><a href="sensitivity-analysis.html#range-of-optimality---constraints"><i class="fa fa-check"></i><b>9.2</b> Range of Optimality - Constraints</a></li>
<li class="chapter" data-level="9.3" data-path="sensitivity-analysis.html"><a href="sensitivity-analysis.html#shadow-prices"><i class="fa fa-check"></i><b>9.3</b> Shadow Prices</a></li>
<li class="chapter" data-level="9.4" data-path="sensitivity-analysis.html"><a href="sensitivity-analysis.html#sensitivity-analysis---objective"><i class="fa fa-check"></i><b>9.4</b> Sensitivity analysis - Objective</a></li>
</ul></li>
<li class="part"><span><b>III Non-Linear Programming</b></span></li>
<li class="chapter" data-level="10" data-path="convex-programming.html"><a href="convex-programming.html"><i class="fa fa-check"></i><b>10</b> Convex Programming</a></li>
<li class="chapter" data-level="11" data-path="separation-theorems.html"><a href="separation-theorems.html"><i class="fa fa-check"></i><b>11</b> Separation Theorems</a>
<ul>
<li class="chapter" data-level="11.1" data-path="separation-theorems.html"><a href="separation-theorems.html#farkas-lemma"><i class="fa fa-check"></i><b>11.1</b> Farkas’ Lemma</a></li>
<li class="chapter" data-level="11.2" data-path="separation-theorems.html"><a href="separation-theorems.html#separating-hyperplane-theorem"><i class="fa fa-check"></i><b>11.2</b> Separating Hyperplane Theorem</a></li>
<li class="chapter" data-level="11.3" data-path="separation-theorems.html"><a href="separation-theorems.html#equivalence-with-strong-duality"><i class="fa fa-check"></i><b>11.3</b> Equivalence with Strong Duality</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="interior-point-method.html"><a href="interior-point-method.html"><i class="fa fa-check"></i><b>12</b> Interior Point Method</a>
<ul>
<li class="chapter" data-level="12.1" data-path="interior-point-method.html"><a href="interior-point-method.html#gradient-descent"><i class="fa fa-check"></i><b>12.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="12.2" data-path="interior-point-method.html"><a href="interior-point-method.html#interior-point-method-1"><i class="fa fa-check"></i><b>12.2</b> Interior Point Method</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="kkt-conditions.html"><a href="kkt-conditions.html"><i class="fa fa-check"></i><b>13</b> KKT conditions</a></li>
<li class="part"><span><b>IV Applications</b></span></li>
<li class="chapter" data-level="14" data-path="l1-regression.html"><a href="l1-regression.html"><i class="fa fa-check"></i><b>14</b> L1-Regression</a></li>
<li class="chapter" data-level="15" data-path="network-flow.html"><a href="network-flow.html"><i class="fa fa-check"></i><b>15</b> Network Flow</a>
<ul>
<li class="chapter" data-level="15.1" data-path="network-flow.html"><a href="network-flow.html#min-cut"><i class="fa fa-check"></i><b>15.1</b> Min-cut</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="integer-programming.html"><a href="integer-programming.html"><i class="fa fa-check"></i><b>16</b> Integer Programming</a>
<ul>
<li class="chapter" data-level="16.1" data-path="integer-programming.html"><a href="integer-programming.html#branch-and-bound"><i class="fa fa-check"></i><b>16.1</b> Branch and Bound</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Optimization</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="interior-point-method" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">Chapter 12</span> Interior Point Method<a href="interior-point-method.html#interior-point-method" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Throughout this chapter we will let <span class="math inline">\(f\)</span> denote a twice differentiable function from <span class="math inline">\(\mathbb{R}^n\)</span> to <span class="math inline">\(\mathbb{R}\)</span>.</p>
<div id="gradient-descent" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> Gradient Descent<a href="interior-point-method.html#gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will start by trying to solve the following unconstrained optimization problem:
<span class="math display">\[\begin{align*}
  \mbox{minimize: } &amp; f(x)
\end{align*}\]</span>
where <span class="math inline">\(x\)</span> is any vector in <span class="math inline">\(\mathbb{R}^n\)</span>.
<strong>Gradient descent</strong> is a simple algorithm for solving this problem using basic differential calculus.
It relies on the fact that <em>the negative of the gradient points in the direction in which the function decreases the fastest</em>.
So the general principle is to move in the direction of the negative gradient until the function is no longer decreasing.
More precisely, we create a sequence of guesses using the following GD recurrence relation:
<span class="math display">\[x_{k+1} = x_k - t_k \nabla f(x_k)\]</span>
where <span class="math inline">\(t_k\)</span> is the “step size” for the <span class="math inline">\(k^{th}\)</span> iteration and can be chosen to be a small constant or some function of <span class="math inline">\(k\)</span>, <span class="math inline">\(x_k\)</span>, <span class="math inline">\(f(x_k)\)</span>, or <span class="math inline">\(\nabla f(x_k)\)</span>.</p>
<p>There are several issues with this technique:</p>
<ol style="list-style-type: decimal">
<li>If the function has multiple local minima, then the sequence might converge to a non-absolute minima depending on the starting guess and the choice of step sizes.</li>
<li>If the step sizes are chosen to be too large, then the sequence can completely miss the minima and may not converge.</li>
<li>If the step sizes are chosen to be too small, then the sequence may take a long time to converge.</li>
</ol>
<p>Unfortunately, there is no easy way to resolve these issues. In practice, either we need some additional information about the function and its gradient or we proceed by trial and error to find step sizes that work. Even then there is no guarantee that the algorithm will converge to an absolute minima and not a local minima, if at all. In spite of these issues, because of its simplicity, ease of implementation, and good convergence properties in practice, Gradient Descent is a very popular algorithm for solving unconstrained optimization problems.</p>
</div>
<div id="interior-point-method-1" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span> Interior Point Method<a href="interior-point-method.html#interior-point-method-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Gradient descent can be modified to solve constrained optimization problems by introducing barrier functions.
Consider the following problem:
<span class="math display" id="eq:constrained-optimization-problem">\[\begin{equation}
  \begin{array}{llr}
    \mbox{minimize: } &amp; f(x) \\
    \mbox{subject to: } &amp; g_i(x) \ge 0, &amp; \mbox{ for } 1 \le i \le m. \\
  \end{array}
  \tag{12.1}
\end{equation}\]</span>
We can apply GD to this problem and find a critical point for <span class="math inline">\(f(x)\)</span>. However, this might not answer the optimization question for two reasons:</p>
<ol style="list-style-type: decimal">
<li>The critical point might not be in the feasible region.</li>
<li>The optimal solution might not be obtained at a critical of <span class="math inline">\(f(x)\)</span> and could lie on the boundary <span class="math inline">\(g(x) = 0\)</span> of the feasible region.</li>
</ol>
<p>Gradient descent algorithm only <em>sees</em> the objective function and does not <em>know</em> about the constraints. So, we modify the objective function to include the constraints using barrier functions.</p>
<div class="definition">
<p><span id="def:unlabeled-div-69" class="definition"><strong>Definition 12.1  </strong></span>A <strong>barrier function</strong> is a differentiable function <span class="math inline">\(b\)</span> from <span class="math inline">\((0, \infty)\)</span> to <span class="math inline">\(\mathbb{R}\)</span> that has the property <span class="math inline">\(\lim \limits_{x \to 0^+} f(x) = \infty\)</span>.</p>
</div>
<p>We’ll use the barrier function <span class="math inline">\(-\ln x\)</span>. Using a small positive parameter <span class="math inline">\(\mu\)</span>, we create a new objective function:
<span class="math display">\[\begin{align*}
  f_\mu(x) := &amp; f(x) - \mu \sum \limits_{i = 1} ^ m \ln (g_i(x)).
\end{align*}\]</span>
Consider the unconstrained optimization problem
<span class="math display">\[\begin{align*}
  \mbox{minimize: } &amp; f_\mu (x).
\end{align*}\]</span>
Because the domain of <span class="math inline">\(\ln x\)</span> is <span class="math inline">\((0, \infty)\)</span>, any critical point of <span class="math inline">\(f_\mu(x)\)</span> must lie in the feasible region of <a href="interior-point-method.html#eq:constrained-optimization-problem">(12.1)</a>.
Let <span class="math inline">\(x_\mu^*\)</span> be a solution of the above problem.
For sufficiently good functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g_i\)</span>, <em><span class="math inline">\(x_\mu^*\)</span> exists and is continuous in <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\lim \limits_{\mu \to 0^+} x_\mu^* = x^*\)</span> where <span class="math inline">\(x^*\)</span> is an optimal solution to <a href="interior-point-method.html#eq:constrained-optimization-problem">(12.1)</a>.</em> This assumption is valid, for example, for linear programs. With this assumption, we now have a method for solving <a href="interior-point-method.html#eq:constrained-optimization-problem">(12.1)</a>:</p>
<ul>
<li>Start with a small <span class="math inline">\(\mu_0 &gt; 0\)</span> and optimize <span class="math inline">\(f_{\mu_0}(x)\)</span> using GD. Call the solution <span class="math inline">\(x_{\mu_0}^*\)</span>.</li>
<li>Repeat the following until the sequence <span class="math inline">\(x_{\mu_k}^*\)</span> stabilizes sufficiently:
<ol style="list-style-type: decimal">
<li>Set <span class="math inline">\(\mu_{k+1} = \mu_k - \delta_k\)</span> for some small <span class="math inline">\(\delta_k\)</span>.</li>
<li>Using <span class="math inline">\(x^*_{\mu_k}\)</span> as a starting point for GD, optimize <span class="math inline">\(f_{\mu_{k+1}}(x)\)</span>. Call the solution <span class="math inline">\(x_{\mu_{k+1}}^*\)</span>.</li>
<li>Increase <span class="math inline">\(k\)</span> to <span class="math inline">\(k + 1\)</span>.</li>
</ol></li>
</ul>
<p>This method is a very simplified <strong>interior point method</strong>. The sequence of points <span class="math inline">\(x_{\mu}\)</span> is called the <strong>central path</strong>.</p>
<div class="example">
<p><span id="exm:ip-kkt" class="example"><strong>Example 12.1  </strong></span>We can compute the central path explicitly for some simple examples. Consider the optimization problem:
<span class="math display" id="eq:ip">\[\begin{equation}
  \begin{split}
  \mbox{minimize: } &amp; (x + 1)^2 \\
  \mbox{subject to: } &amp; x \ge 0.
  \end{split}
  \tag{12.2}
\end{equation}\]</span>
We can calculate the central path as follows:
<span class="math display">\[\begin{align*}
  f_\mu(x) := (x + 1)^2 - \mu \ln (x).
\end{align*}\]</span>
The critical points for this function can be obtained by setting the derivative to 0.
<span class="math display">\[\begin{align*}
  f&#39;_\mu(x) &amp;= 0 \\ 
  \implies 2(x + 1) - \dfrac{\mu}{x} &amp;= 0 \\
  \implies x^2 + x - \mu/2 &amp;= 0 \\
  \implies x &amp;= \dfrac{-1 \pm \sqrt{1 + 2 \mu}}{2}.
\end{align*}\]</span>
Only one of the two satisfy <span class="math inline">\(x \ge 0\)</span>, so we get
<span class="math display">\[\begin{align*}
  x_\mu^* = \dfrac{-1 + \sqrt{1 + 2 \mu}}{2}. 
\end{align*}\]</span>
This is the central path. Taking the limit as <span class="math inline">\(\mu \to 0\)</span> we get,
<span class="math display">\[\begin{align*}
  \lim \limits_{\mu \to 0^+} x_\mu^* 
  &amp; = \lim \limits_{\mu \to 0^+} \dfrac{-1 + \sqrt{1 + 2 \mu}}{2} \\ 
  &amp;= 0,
\end{align*}\]</span>
which is indeed the optimal solution for the optimization problem <a href="interior-point-method.html#eq:ip">(12.2)</a>.</p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="separation-theorems.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="kkt-conditions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": "github"
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Introduction to Optimization.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
